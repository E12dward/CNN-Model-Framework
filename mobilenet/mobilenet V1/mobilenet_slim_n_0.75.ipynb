{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D,GlobalAveragePooling2D, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import DepthwiseConv2D\n",
    "\n",
    "from keras import optimizers,regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "\n",
    "num_classes        = 10\n",
    "batch_size         = 64         # 64 or 32 or other\n",
    "epochs             = 300\n",
    "iterations         = 782       \n",
    "USE_BN=True\n",
    "DROPOUT=0.2 # keep 80%\n",
    "CONCAT_AXIS=3\n",
    "weight_decay=1e-4\n",
    "DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'\n",
    "alpha = 0.75\n",
    "\n",
    "log_filepath  = './mobilenet_slim_n_0.75'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.01\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    return 0.0001\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depthwise_separable(x,params):\n",
    "    # f1/f2 filter size, s1 stride of conv\n",
    "    (s1,f2) = params\n",
    "    x = DepthwiseConv2D((3,3),strides=(s1[0],s1[0]), padding='same',\n",
    "                        depthwise_initializer=\"he_normal\",depthwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(int(alpha*f2[0]), (1,1), strides=(1,1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MobileNet(img_input,shallow=False, classes=10):\n",
    "    \"\"\"Instantiates the MobileNet.Network has two hyper-parameters\n",
    "        which are the width of network (controlled by alpha)\n",
    "        and input size.\n",
    "        # Arguments\n",
    "            alpha: optional parameter of the network to change the \n",
    "                width of model.\n",
    "            shallow: optional parameter for making network smaller.\n",
    "            classes: optional number of classes to classify images\n",
    "                into.\n",
    "    \"\"\"\n",
    "    # change stride\n",
    "    x = Conv2D(int(alpha*32), (3,3), strides=(1,1), padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = depthwise_separable(x,params=[(1,),(64,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(128,)])# change stride\n",
    "    x = depthwise_separable(x,params=[(1,),(128,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(256,)])# change stride\n",
    "    x = depthwise_separable(x,params=[(1,),(256,)])\n",
    "    x = depthwise_separable(x,params=[(2,),(512,)])\n",
    "    if not shallow:\n",
    "        for _ in range(5):\n",
    "            x = depthwise_separable(x,params=[(1,),(512,)])\n",
    "            \n",
    "    x = depthwise_separable(x,params=[(2,),(1024,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(1024,)])\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    out = Dense(classes, activation='softmax')(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 24)        672       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 24)        96        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 24)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 32, 32, 24)        240       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 24)        96        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 48)        1200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 32, 32, 48)        480       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 96)        4704      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 32, 32, 96)        960       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 96)        9312      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 32, 32, 96)        960       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 192)       18624     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 192)       768       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 32, 32, 192)       1920      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 32, 192)       768       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 192)       37056     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 192)       768       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 32, 192)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 16, 16, 192)       1920      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 192)       768       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16, 16, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 384)       74112     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_7 (Depthwis (None, 16, 16, 384)       3840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_8 (Depthwis (None, 16, 16, 384)       3840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_9 (Depthwis (None, 16, 16, 384)       3840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_10 (Depthwi (None, 16, 16, 384)       3840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_11 (Depthwi (None, 16, 16, 384)       3840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 384)       147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 16, 16, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16, 16, 384)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_12 (Depthwi (None, 8, 8, 384)         3840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 8, 8, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8, 8, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 768)         295680    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 8, 8, 768)         3072      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8, 8, 768)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_13 (Depthwi (None, 8, 8, 768)         7680      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 8, 8, 768)         3072      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 8, 8, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 768)         590592    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 8, 8, 768)         3072      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 8, 8, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7690      \n",
      "=================================================================\n",
      "Total params: 1,848,874\n",
      "Trainable params: 1,832,458\n",
      "Non-trainable params: 16,416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input=Input(shape=(32,32,3))\n",
    "output = MobileNet(img_input)\n",
    "model=Model(img_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 76s 97ms/step - loss: 2.3349 - acc: 0.3950 - val_loss: 2.1997 - val_acc: 0.4803\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 71s 90ms/step - loss: 1.9465 - acc: 0.5466 - val_loss: 1.8141 - val_acc: 0.6046\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.7108 - acc: 0.6342 - val_loss: 1.6270 - val_acc: 0.6659\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.5278 - acc: 0.6962 - val_loss: 1.5702 - val_acc: 0.6862\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.3991 - acc: 0.7396 - val_loss: 1.4113 - val_acc: 0.7402\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.2933 - acc: 0.7716 - val_loss: 1.3547 - val_acc: 0.7640\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.2169 - acc: 0.7943 - val_loss: 1.3152 - val_acc: 0.7670\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.1537 - acc: 0.8133 - val_loss: 1.2635 - val_acc: 0.7854\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.1005 - acc: 0.8251 - val_loss: 1.2433 - val_acc: 0.7885\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 1.0485 - acc: 0.8396 - val_loss: 1.1879 - val_acc: 0.8048\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.0056 - acc: 0.8522 - val_loss: 1.0920 - val_acc: 0.8301\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.9707 - acc: 0.8587 - val_loss: 1.1021 - val_acc: 0.8261\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.9285 - acc: 0.8698 - val_loss: 1.1308 - val_acc: 0.8152\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.9032 - acc: 0.8746 - val_loss: 1.0910 - val_acc: 0.8284\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.8683 - acc: 0.8829 - val_loss: 1.0187 - val_acc: 0.8406\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.8404 - acc: 0.8897 - val_loss: 0.9979 - val_acc: 0.8443\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.8104 - acc: 0.8978 - val_loss: 0.9966 - val_acc: 0.8455\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.7892 - acc: 0.8997 - val_loss: 0.9826 - val_acc: 0.8476\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.7653 - acc: 0.9060 - val_loss: 1.0059 - val_acc: 0.8446\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.7427 - acc: 0.9114 - val_loss: 0.9729 - val_acc: 0.8490\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.7274 - acc: 0.9118 - val_loss: 0.9498 - val_acc: 0.8573\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.7067 - acc: 0.9183 - val_loss: 0.9238 - val_acc: 0.8598\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.6876 - acc: 0.9200 - val_loss: 0.9439 - val_acc: 0.8584\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.6661 - acc: 0.9246 - val_loss: 0.8760 - val_acc: 0.8703\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.6527 - acc: 0.9277 - val_loss: 0.8904 - val_acc: 0.8719\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.6419 - acc: 0.9286 - val_loss: 0.9329 - val_acc: 0.8581\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.6233 - acc: 0.9321 - val_loss: 0.8882 - val_acc: 0.8659\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.6082 - acc: 0.9349 - val_loss: 0.9086 - val_acc: 0.8641\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.5950 - acc: 0.9372 - val_loss: 0.8939 - val_acc: 0.8605\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.5854 - acc: 0.9391 - val_loss: 0.8714 - val_acc: 0.8711\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.5711 - acc: 0.9411 - val_loss: 0.9796 - val_acc: 0.8489\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.5618 - acc: 0.9434 - val_loss: 0.9040 - val_acc: 0.8645\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.5505 - acc: 0.9446 - val_loss: 0.8293 - val_acc: 0.8741\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.5381 - acc: 0.9474 - val_loss: 0.8322 - val_acc: 0.8803\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.5275 - acc: 0.9486 - val_loss: 0.8322 - val_acc: 0.8812\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.5178 - acc: 0.9506 - val_loss: 0.7927 - val_acc: 0.8811\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.5040 - acc: 0.9533 - val_loss: 0.8345 - val_acc: 0.8709\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.5034 - acc: 0.9521 - val_loss: 0.8115 - val_acc: 0.8819\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.4864 - acc: 0.9559 - val_loss: 0.8500 - val_acc: 0.8708\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.4817 - acc: 0.9557 - val_loss: 0.7961 - val_acc: 0.8841\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4709 - acc: 0.9584 - val_loss: 0.8053 - val_acc: 0.8786\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4673 - acc: 0.9573 - val_loss: 0.8183 - val_acc: 0.8801\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4617 - acc: 0.9580 - val_loss: 0.8112 - val_acc: 0.8794\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.4557 - acc: 0.9593 - val_loss: 0.7890 - val_acc: 0.8829\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4488 - acc: 0.9593 - val_loss: 0.8055 - val_acc: 0.8831\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4376 - acc: 0.9621 - val_loss: 0.7623 - val_acc: 0.8873\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4367 - acc: 0.9610 - val_loss: 0.7608 - val_acc: 0.8829\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.4299 - acc: 0.9637 - val_loss: 0.7813 - val_acc: 0.8830\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.4205 - acc: 0.9646 - val_loss: 0.7533 - val_acc: 0.8894\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.4168 - acc: 0.9641 - val_loss: 0.7560 - val_acc: 0.8868\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.4104 - acc: 0.9651 - val_loss: 0.8038 - val_acc: 0.8757\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.4043 - acc: 0.9658 - val_loss: 0.7937 - val_acc: 0.8738\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.4103 - acc: 0.9631 - val_loss: 0.7769 - val_acc: 0.8798\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.3913 - acc: 0.9682 - val_loss: 0.7552 - val_acc: 0.8821\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3932 - acc: 0.9660 - val_loss: 0.7268 - val_acc: 0.8894\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3855 - acc: 0.9682 - val_loss: 0.7850 - val_acc: 0.8796\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.3911 - acc: 0.9645 - val_loss: 0.7564 - val_acc: 0.8820\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3740 - acc: 0.9705 - val_loss: 0.7404 - val_acc: 0.8861\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3772 - acc: 0.9679 - val_loss: 0.7636 - val_acc: 0.8800\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3716 - acc: 0.9685 - val_loss: 0.7762 - val_acc: 0.8826\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3611 - acc: 0.9716 - val_loss: 0.7895 - val_acc: 0.8824\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3658 - acc: 0.9692 - val_loss: 0.7923 - val_acc: 0.8822\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3675 - acc: 0.9675 - val_loss: 0.7285 - val_acc: 0.8884\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3540 - acc: 0.9720 - val_loss: 0.7100 - val_acc: 0.8908\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3522 - acc: 0.9707 - val_loss: 0.8532 - val_acc: 0.8713\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3460 - acc: 0.9722 - val_loss: 0.7009 - val_acc: 0.8937\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3505 - acc: 0.9702 - val_loss: 0.7573 - val_acc: 0.8806\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3391 - acc: 0.9731 - val_loss: 0.6929 - val_acc: 0.8949\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3399 - acc: 0.9731 - val_loss: 0.7360 - val_acc: 0.8887\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3417 - acc: 0.9709 - val_loss: 0.7475 - val_acc: 0.8856\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.3383 - acc: 0.9713 - val_loss: 0.6844 - val_acc: 0.8943\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3329 - acc: 0.9723 - val_loss: 0.7135 - val_acc: 0.8927\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.3272 - acc: 0.9735 - val_loss: 0.7506 - val_acc: 0.8878\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3260 - acc: 0.9731 - val_loss: 0.6791 - val_acc: 0.8951\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3163 - acc: 0.9767 - val_loss: 0.6866 - val_acc: 0.8957\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3213 - acc: 0.9726 - val_loss: 0.7653 - val_acc: 0.8853\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3200 - acc: 0.9736 - val_loss: 0.7277 - val_acc: 0.8924\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3261 - acc: 0.9709 - val_loss: 0.7010 - val_acc: 0.8958\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3215 - acc: 0.9718 - val_loss: 0.6968 - val_acc: 0.8928\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3194 - acc: 0.9730 - val_loss: 0.6970 - val_acc: 0.8912\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3137 - acc: 0.9738 - val_loss: 0.6784 - val_acc: 0.8972\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.3014 - acc: 0.9771 - val_loss: 0.7015 - val_acc: 0.8916\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3048 - acc: 0.9753 - val_loss: 0.6759 - val_acc: 0.9013\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.3030 - acc: 0.9755 - val_loss: 0.7144 - val_acc: 0.8947\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.3071 - acc: 0.9736 - val_loss: 0.6637 - val_acc: 0.8998\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2989 - acc: 0.9766 - val_loss: 0.7294 - val_acc: 0.8883\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2988 - acc: 0.9758 - val_loss: 0.6847 - val_acc: 0.8948\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.2925 - acc: 0.9770 - val_loss: 0.6705 - val_acc: 0.8969\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2911 - acc: 0.9759 - val_loss: 0.6641 - val_acc: 0.8954\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2881 - acc: 0.9781 - val_loss: 0.7003 - val_acc: 0.8918\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2800 - acc: 0.9801 - val_loss: 0.7685 - val_acc: 0.8839\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2862 - acc: 0.9773 - val_loss: 0.6903 - val_acc: 0.8954\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.2924 - acc: 0.9751 - val_loss: 0.6572 - val_acc: 0.8994\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.2806 - acc: 0.9783 - val_loss: 0.6854 - val_acc: 0.8966\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2893 - acc: 0.9750 - val_loss: 0.6744 - val_acc: 0.8912\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2810 - acc: 0.9774 - val_loss: 0.6289 - val_acc: 0.8992\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.2790 - acc: 0.9780 - val_loss: 0.6790 - val_acc: 0.8960\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2770 - acc: 0.9777 - val_loss: 0.6510 - val_acc: 0.9003\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.2759 - acc: 0.9781 - val_loss: 0.6708 - val_acc: 0.8967\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2708 - acc: 0.9790 - val_loss: 0.6903 - val_acc: 0.8953\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.2459 - acc: 0.9885 - val_loss: 0.5746 - val_acc: 0.9151\n",
      "Epoch 102/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2292 - acc: 0.9943 - val_loss: 0.5637 - val_acc: 0.9176\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2234 - acc: 0.9967 - val_loss: 0.5624 - val_acc: 0.9155\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2206 - acc: 0.9971 - val_loss: 0.5572 - val_acc: 0.9181\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2187 - acc: 0.9972 - val_loss: 0.5597 - val_acc: 0.9189\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2164 - acc: 0.9983 - val_loss: 0.5603 - val_acc: 0.9192\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2160 - acc: 0.9978 - val_loss: 0.5564 - val_acc: 0.9190\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2141 - acc: 0.9982 - val_loss: 0.5568 - val_acc: 0.9200\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2126 - acc: 0.9986 - val_loss: 0.5629 - val_acc: 0.9197\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 72s 92ms/step - loss: 0.2120 - acc: 0.9985 - val_loss: 0.5613 - val_acc: 0.9205\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2107 - acc: 0.9986 - val_loss: 0.5651 - val_acc: 0.9190\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2104 - acc: 0.9985 - val_loss: 0.5627 - val_acc: 0.9199\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2088 - acc: 0.9989 - val_loss: 0.5626 - val_acc: 0.9204\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2080 - acc: 0.9990 - val_loss: 0.5653 - val_acc: 0.9185\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2068 - acc: 0.9991 - val_loss: 0.5653 - val_acc: 0.9211\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2061 - acc: 0.9991 - val_loss: 0.5679 - val_acc: 0.9194\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2057 - acc: 0.9989 - val_loss: 0.5615 - val_acc: 0.9205\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2047 - acc: 0.9993 - val_loss: 0.5681 - val_acc: 0.9192\n",
      "Epoch 119/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2043 - acc: 0.9991 - val_loss: 0.5659 - val_acc: 0.9207\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2029 - acc: 0.9993 - val_loss: 0.5686 - val_acc: 0.9196\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 69s 88ms/step - loss: 0.2023 - acc: 0.9992 - val_loss: 0.5639 - val_acc: 0.9204\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2013 - acc: 0.9993 - val_loss: 0.5688 - val_acc: 0.9189\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2010 - acc: 0.9992 - val_loss: 0.5689 - val_acc: 0.9216\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.2002 - acc: 0.9991 - val_loss: 0.5699 - val_acc: 0.9200\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1997 - acc: 0.9993 - val_loss: 0.5690 - val_acc: 0.9207\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1991 - acc: 0.9991 - val_loss: 0.5636 - val_acc: 0.9218\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1981 - acc: 0.9993 - val_loss: 0.5647 - val_acc: 0.9222\n",
      "Epoch 128/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1973 - acc: 0.9994 - val_loss: 0.5646 - val_acc: 0.9220\n",
      "Epoch 129/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1968 - acc: 0.9994 - val_loss: 0.5661 - val_acc: 0.9214\n",
      "Epoch 130/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1958 - acc: 0.9995 - val_loss: 0.5608 - val_acc: 0.9232\n",
      "Epoch 131/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1952 - acc: 0.9995 - val_loss: 0.5680 - val_acc: 0.9219\n",
      "Epoch 132/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1944 - acc: 0.9995 - val_loss: 0.5649 - val_acc: 0.9223\n",
      "Epoch 133/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1940 - acc: 0.9995 - val_loss: 0.5648 - val_acc: 0.9222\n",
      "Epoch 134/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1933 - acc: 0.9995 - val_loss: 0.5633 - val_acc: 0.9217\n",
      "Epoch 135/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1925 - acc: 0.9994 - val_loss: 0.5632 - val_acc: 0.9225\n",
      "Epoch 136/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1919 - acc: 0.9996 - val_loss: 0.5649 - val_acc: 0.9216\n",
      "Epoch 137/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1913 - acc: 0.9996 - val_loss: 0.5638 - val_acc: 0.9223\n",
      "Epoch 138/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1906 - acc: 0.9996 - val_loss: 0.5616 - val_acc: 0.9224\n",
      "Epoch 139/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1899 - acc: 0.9997 - val_loss: 0.5686 - val_acc: 0.9225\n",
      "Epoch 140/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1896 - acc: 0.9996 - val_loss: 0.5656 - val_acc: 0.9219\n",
      "Epoch 141/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1887 - acc: 0.9996 - val_loss: 0.5670 - val_acc: 0.9216\n",
      "Epoch 142/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1880 - acc: 0.9996 - val_loss: 0.5691 - val_acc: 0.9211\n",
      "Epoch 143/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1874 - acc: 0.9996 - val_loss: 0.5768 - val_acc: 0.9210\n",
      "Epoch 144/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1872 - acc: 0.9995 - val_loss: 0.5712 - val_acc: 0.9221\n",
      "Epoch 145/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1861 - acc: 0.9997 - val_loss: 0.5690 - val_acc: 0.9215\n",
      "Epoch 146/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1856 - acc: 0.9997 - val_loss: 0.5695 - val_acc: 0.9221\n",
      "Epoch 147/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1851 - acc: 0.9996 - val_loss: 0.5739 - val_acc: 0.9200\n",
      "Epoch 148/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1845 - acc: 0.9996 - val_loss: 0.5603 - val_acc: 0.9226\n",
      "Epoch 149/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1839 - acc: 0.9996 - val_loss: 0.5692 - val_acc: 0.9215\n",
      "Epoch 150/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1832 - acc: 0.9997 - val_loss: 0.5632 - val_acc: 0.9236\n",
      "Epoch 151/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1827 - acc: 0.9997 - val_loss: 0.5659 - val_acc: 0.9222\n",
      "Epoch 152/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1821 - acc: 0.9997 - val_loss: 0.5712 - val_acc: 0.9220\n",
      "Epoch 153/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1814 - acc: 0.9997 - val_loss: 0.5658 - val_acc: 0.9230\n",
      "Epoch 154/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1807 - acc: 0.9998 - val_loss: 0.5697 - val_acc: 0.9230\n",
      "Epoch 155/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1806 - acc: 0.9995 - val_loss: 0.5621 - val_acc: 0.9225\n",
      "Epoch 156/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1798 - acc: 0.9996 - val_loss: 0.5693 - val_acc: 0.9211\n",
      "Epoch 157/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1792 - acc: 0.9997 - val_loss: 0.5716 - val_acc: 0.9199\n",
      "Epoch 158/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1785 - acc: 0.9998 - val_loss: 0.5640 - val_acc: 0.9225\n",
      "Epoch 159/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1777 - acc: 0.9998 - val_loss: 0.5657 - val_acc: 0.9225\n",
      "Epoch 160/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1775 - acc: 0.9996 - val_loss: 0.5665 - val_acc: 0.9228\n",
      "Epoch 161/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1767 - acc: 0.9998 - val_loss: 0.5604 - val_acc: 0.9234\n",
      "Epoch 162/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1763 - acc: 0.9997 - val_loss: 0.5669 - val_acc: 0.9216\n",
      "Epoch 163/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1757 - acc: 0.9998 - val_loss: 0.5642 - val_acc: 0.9235\n",
      "Epoch 164/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1752 - acc: 0.9997 - val_loss: 0.5623 - val_acc: 0.9225\n",
      "Epoch 165/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1746 - acc: 0.9997 - val_loss: 0.5631 - val_acc: 0.9221\n",
      "Epoch 166/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1741 - acc: 0.9997 - val_loss: 0.5608 - val_acc: 0.9248\n",
      "Epoch 167/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1736 - acc: 0.9997 - val_loss: 0.5670 - val_acc: 0.9215\n",
      "Epoch 168/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1728 - acc: 0.9999 - val_loss: 0.5571 - val_acc: 0.9243\n",
      "Epoch 169/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1721 - acc: 0.9999 - val_loss: 0.5629 - val_acc: 0.9234\n",
      "Epoch 170/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1716 - acc: 0.9999 - val_loss: 0.5572 - val_acc: 0.9232\n",
      "Epoch 171/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1715 - acc: 0.9997 - val_loss: 0.5586 - val_acc: 0.9245\n",
      "Epoch 172/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1705 - acc: 0.9998 - val_loss: 0.5631 - val_acc: 0.9227\n",
      "Epoch 173/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1701 - acc: 0.9997 - val_loss: 0.5626 - val_acc: 0.9233\n",
      "Epoch 174/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1699 - acc: 0.9997 - val_loss: 0.5578 - val_acc: 0.9231\n",
      "Epoch 175/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1693 - acc: 0.9997 - val_loss: 0.5586 - val_acc: 0.9231\n",
      "Epoch 176/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1685 - acc: 0.9998 - val_loss: 0.5552 - val_acc: 0.9240\n",
      "Epoch 177/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1679 - acc: 0.9999 - val_loss: 0.5634 - val_acc: 0.9237\n",
      "Epoch 178/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1676 - acc: 0.9998 - val_loss: 0.5596 - val_acc: 0.9235\n",
      "Epoch 179/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1670 - acc: 0.9998 - val_loss: 0.5535 - val_acc: 0.9235\n",
      "Epoch 180/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1663 - acc: 0.9998 - val_loss: 0.5555 - val_acc: 0.9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1659 - acc: 0.9998 - val_loss: 0.5615 - val_acc: 0.9226\n",
      "Epoch 182/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1655 - acc: 0.9998 - val_loss: 0.5577 - val_acc: 0.9240\n",
      "Epoch 183/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1649 - acc: 0.9998 - val_loss: 0.5559 - val_acc: 0.9245\n",
      "Epoch 184/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1644 - acc: 0.9998 - val_loss: 0.5617 - val_acc: 0.9223\n",
      "Epoch 185/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1639 - acc: 0.9997 - val_loss: 0.5646 - val_acc: 0.9221\n",
      "Epoch 186/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1632 - acc: 0.9998 - val_loss: 0.5532 - val_acc: 0.9243\n",
      "Epoch 187/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1627 - acc: 0.9998 - val_loss: 0.5544 - val_acc: 0.9237\n",
      "Epoch 188/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1623 - acc: 0.9998 - val_loss: 0.5603 - val_acc: 0.9236\n",
      "Epoch 189/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1618 - acc: 0.9998 - val_loss: 0.5520 - val_acc: 0.9232\n",
      "Epoch 190/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1612 - acc: 0.9998 - val_loss: 0.5461 - val_acc: 0.9244\n",
      "Epoch 191/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1604 - acc: 0.9999 - val_loss: 0.5514 - val_acc: 0.9245\n",
      "Epoch 192/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1600 - acc: 0.9998 - val_loss: 0.5501 - val_acc: 0.9240\n",
      "Epoch 193/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1596 - acc: 0.9998 - val_loss: 0.5583 - val_acc: 0.9238\n",
      "Epoch 194/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1597 - acc: 0.9996 - val_loss: 0.5542 - val_acc: 0.9234\n",
      "Epoch 195/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1588 - acc: 0.9998 - val_loss: 0.5640 - val_acc: 0.9231\n",
      "Epoch 196/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1581 - acc: 0.9998 - val_loss: 0.5531 - val_acc: 0.9246\n",
      "Epoch 197/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1577 - acc: 0.9999 - val_loss: 0.5514 - val_acc: 0.9239\n",
      "Epoch 198/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1571 - acc: 0.9998 - val_loss: 0.5627 - val_acc: 0.9223\n",
      "Epoch 199/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1566 - acc: 0.9999 - val_loss: 0.5555 - val_acc: 0.9231\n",
      "Epoch 200/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1562 - acc: 0.9998 - val_loss: 0.5494 - val_acc: 0.9247\n",
      "Epoch 201/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1558 - acc: 0.9999 - val_loss: 0.5484 - val_acc: 0.9253\n",
      "Epoch 202/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1559 - acc: 0.9998 - val_loss: 0.5520 - val_acc: 0.9238\n",
      "Epoch 203/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1558 - acc: 0.9999 - val_loss: 0.5487 - val_acc: 0.9244\n",
      "Epoch 204/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1557 - acc: 0.9999 - val_loss: 0.5476 - val_acc: 0.9235\n",
      "Epoch 205/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1555 - acc: 0.9999 - val_loss: 0.5475 - val_acc: 0.9233\n",
      "Epoch 206/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1557 - acc: 0.9998 - val_loss: 0.5506 - val_acc: 0.9238\n",
      "Epoch 207/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1556 - acc: 0.9998 - val_loss: 0.5543 - val_acc: 0.9230\n",
      "Epoch 208/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1554 - acc: 0.9999 - val_loss: 0.5494 - val_acc: 0.9228\n",
      "Epoch 209/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1554 - acc: 0.9998 - val_loss: 0.5453 - val_acc: 0.9254\n",
      "Epoch 210/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1556 - acc: 0.9999 - val_loss: 0.5598 - val_acc: 0.9237\n",
      "Epoch 211/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1554 - acc: 0.9998 - val_loss: 0.5557 - val_acc: 0.9228\n",
      "Epoch 212/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1552 - acc: 0.9999 - val_loss: 0.5485 - val_acc: 0.9238\n",
      "Epoch 213/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1553 - acc: 0.9998 - val_loss: 0.5510 - val_acc: 0.9237\n",
      "Epoch 214/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1551 - acc: 0.9999 - val_loss: 0.5502 - val_acc: 0.9238\n",
      "Epoch 215/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1552 - acc: 0.9998 - val_loss: 0.5500 - val_acc: 0.9244\n",
      "Epoch 216/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1551 - acc: 0.9999 - val_loss: 0.5525 - val_acc: 0.9239\n",
      "Epoch 217/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1551 - acc: 0.9998 - val_loss: 0.5513 - val_acc: 0.9238\n",
      "Epoch 218/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1549 - acc: 0.9998 - val_loss: 0.5462 - val_acc: 0.9253\n",
      "Epoch 219/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1549 - acc: 0.9999 - val_loss: 0.5523 - val_acc: 0.9240\n",
      "Epoch 220/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1549 - acc: 0.9999 - val_loss: 0.5522 - val_acc: 0.9223\n",
      "Epoch 221/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1549 - acc: 0.9999 - val_loss: 0.5503 - val_acc: 0.9246\n",
      "Epoch 222/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1548 - acc: 0.9999 - val_loss: 0.5502 - val_acc: 0.9234\n",
      "Epoch 223/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1548 - acc: 0.9999 - val_loss: 0.5547 - val_acc: 0.9229\n",
      "Epoch 224/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1547 - acc: 0.9998 - val_loss: 0.5544 - val_acc: 0.9229\n",
      "Epoch 225/300\n",
      "782/782 [==============================] - 68s 88ms/step - loss: 0.1548 - acc: 0.9998 - val_loss: 0.5556 - val_acc: 0.9225\n",
      "Epoch 226/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1550 - acc: 0.9998 - val_loss: 0.5536 - val_acc: 0.9228\n",
      "Epoch 227/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1546 - acc: 0.9999 - val_loss: 0.5504 - val_acc: 0.9235\n",
      "Epoch 228/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1543 - acc: 0.9999 - val_loss: 0.5507 - val_acc: 0.9231\n",
      "Epoch 229/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1544 - acc: 0.9999 - val_loss: 0.5494 - val_acc: 0.9239\n",
      "Epoch 230/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1543 - acc: 0.9998 - val_loss: 0.5517 - val_acc: 0.9236\n",
      "Epoch 231/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1544 - acc: 0.9998 - val_loss: 0.5527 - val_acc: 0.9232\n",
      "Epoch 232/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1541 - acc: 0.9999 - val_loss: 0.5505 - val_acc: 0.9245\n",
      "Epoch 233/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1542 - acc: 0.9999 - val_loss: 0.5511 - val_acc: 0.9242\n",
      "Epoch 234/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1541 - acc: 0.9999 - val_loss: 0.5522 - val_acc: 0.9218\n",
      "Epoch 235/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1545 - acc: 0.9996 - val_loss: 0.5569 - val_acc: 0.9240\n",
      "Epoch 236/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1541 - acc: 0.9998 - val_loss: 0.5509 - val_acc: 0.9233\n",
      "Epoch 237/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1542 - acc: 0.9998 - val_loss: 0.5493 - val_acc: 0.9243\n",
      "Epoch 238/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1540 - acc: 0.9999 - val_loss: 0.5484 - val_acc: 0.9242\n",
      "Epoch 239/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1539 - acc: 0.9999 - val_loss: 0.5497 - val_acc: 0.9245\n",
      "Epoch 240/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1538 - acc: 0.9999 - val_loss: 0.5506 - val_acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1538 - acc: 0.9999 - val_loss: 0.5505 - val_acc: 0.9239\n",
      "Epoch 242/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1536 - acc: 1.0000 - val_loss: 0.5518 - val_acc: 0.9238\n",
      "Epoch 243/300\n",
      "782/782 [==============================] - 69s 89ms/step - loss: 0.1536 - acc: 1.0000 - val_loss: 0.5493 - val_acc: 0.9238\n",
      "Epoch 244/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1535 - acc: 0.9999 - val_loss: 0.5492 - val_acc: 0.9247\n",
      "Epoch 245/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1535 - acc: 0.9999 - val_loss: 0.5544 - val_acc: 0.9247\n",
      "Epoch 246/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1535 - acc: 0.9999 - val_loss: 0.5519 - val_acc: 0.9233\n",
      "Epoch 247/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1535 - acc: 0.9999 - val_loss: 0.5525 - val_acc: 0.9245\n",
      "Epoch 248/300\n",
      "782/782 [==============================] - 71s 90ms/step - loss: 0.1535 - acc: 0.9998 - val_loss: 0.5493 - val_acc: 0.9238\n",
      "Epoch 249/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1535 - acc: 0.9998 - val_loss: 0.5485 - val_acc: 0.9244\n",
      "Epoch 250/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1534 - acc: 0.9998 - val_loss: 0.5530 - val_acc: 0.9229\n",
      "Epoch 251/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1532 - acc: 0.9999 - val_loss: 0.5452 - val_acc: 0.9245\n",
      "Epoch 252/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1534 - acc: 0.9998 - val_loss: 0.5492 - val_acc: 0.9245\n",
      "Epoch 253/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1533 - acc: 0.9998 - val_loss: 0.5526 - val_acc: 0.9236\n",
      "Epoch 254/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1532 - acc: 0.9998 - val_loss: 0.5461 - val_acc: 0.9244\n",
      "Epoch 255/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1531 - acc: 0.9998 - val_loss: 0.5479 - val_acc: 0.9252\n",
      "Epoch 256/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1532 - acc: 0.9998 - val_loss: 0.5530 - val_acc: 0.9241\n",
      "Epoch 257/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1530 - acc: 0.9999 - val_loss: 0.5473 - val_acc: 0.9243\n",
      "Epoch 258/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1531 - acc: 0.9999 - val_loss: 0.5511 - val_acc: 0.9240\n",
      "Epoch 259/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1530 - acc: 0.9999 - val_loss: 0.5464 - val_acc: 0.9248\n",
      "Epoch 260/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1529 - acc: 0.9999 - val_loss: 0.5539 - val_acc: 0.9232\n",
      "Epoch 261/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1529 - acc: 0.9999 - val_loss: 0.5481 - val_acc: 0.9249\n",
      "Epoch 262/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1528 - acc: 0.9999 - val_loss: 0.5488 - val_acc: 0.9241\n",
      "Epoch 263/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1526 - acc: 0.9999 - val_loss: 0.5607 - val_acc: 0.9231\n",
      "Epoch 264/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1527 - acc: 0.9999 - val_loss: 0.5505 - val_acc: 0.9229\n",
      "Epoch 265/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1527 - acc: 0.9998 - val_loss: 0.5518 - val_acc: 0.9236\n",
      "Epoch 266/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1526 - acc: 0.9998 - val_loss: 0.5521 - val_acc: 0.9242\n",
      "Epoch 267/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1525 - acc: 0.9999 - val_loss: 0.5481 - val_acc: 0.9252\n",
      "Epoch 268/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1525 - acc: 0.9999 - val_loss: 0.5471 - val_acc: 0.9249\n",
      "Epoch 269/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1524 - acc: 0.9999 - val_loss: 0.5530 - val_acc: 0.9232\n",
      "Epoch 270/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1525 - acc: 0.9998 - val_loss: 0.5512 - val_acc: 0.9230\n",
      "Epoch 271/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1526 - acc: 0.9998 - val_loss: 0.5510 - val_acc: 0.9234\n",
      "Epoch 272/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1522 - acc: 0.9999 - val_loss: 0.5506 - val_acc: 0.9249\n",
      "Epoch 273/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1522 - acc: 0.9999 - val_loss: 0.5567 - val_acc: 0.9235\n",
      "Epoch 274/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1523 - acc: 0.9999 - val_loss: 0.5517 - val_acc: 0.9238\n",
      "Epoch 275/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1523 - acc: 0.9999 - val_loss: 0.5513 - val_acc: 0.9237\n",
      "Epoch 276/300\n",
      "782/782 [==============================] - 71s 90ms/step - loss: 0.1521 - acc: 0.9999 - val_loss: 0.5526 - val_acc: 0.9232\n",
      "Epoch 277/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1522 - acc: 0.9999 - val_loss: 0.5499 - val_acc: 0.9237\n",
      "Epoch 278/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1521 - acc: 0.9999 - val_loss: 0.5454 - val_acc: 0.9240\n",
      "Epoch 279/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1520 - acc: 0.9999 - val_loss: 0.5470 - val_acc: 0.9232\n",
      "Epoch 280/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 0.1520 - acc: 0.9999 - val_loss: 0.5467 - val_acc: 0.9231\n",
      "Epoch 281/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1520 - acc: 0.9998 - val_loss: 0.5504 - val_acc: 0.9236\n",
      "Epoch 282/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1518 - acc: 0.9999 - val_loss: 0.5493 - val_acc: 0.9235\n",
      "Epoch 283/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1520 - acc: 0.9997 - val_loss: 0.5507 - val_acc: 0.9246\n",
      "Epoch 284/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1518 - acc: 0.9999 - val_loss: 0.5479 - val_acc: 0.9240\n",
      "Epoch 285/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1516 - acc: 1.0000 - val_loss: 0.5538 - val_acc: 0.9239\n",
      "Epoch 286/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1516 - acc: 1.0000 - val_loss: 0.5457 - val_acc: 0.9255\n",
      "Epoch 287/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1516 - acc: 0.9999 - val_loss: 0.5512 - val_acc: 0.9239\n",
      "Epoch 288/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1515 - acc: 0.9999 - val_loss: 0.5469 - val_acc: 0.9234\n",
      "Epoch 289/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.1514 - acc: 1.0000 - val_loss: 0.5491 - val_acc: 0.9239\n",
      "Epoch 290/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.1515 - acc: 0.9999 - val_loss: 0.5500 - val_acc: 0.9240\n",
      "Epoch 291/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.1514 - acc: 0.9999 - val_loss: 0.5493 - val_acc: 0.9241\n",
      "Epoch 292/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1513 - acc: 0.9998 - val_loss: 0.5501 - val_acc: 0.9247\n",
      "Epoch 293/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1513 - acc: 0.9999 - val_loss: 0.5485 - val_acc: 0.9236\n",
      "Epoch 294/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1513 - acc: 0.9999 - val_loss: 0.5429 - val_acc: 0.9251\n",
      "Epoch 295/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1512 - acc: 0.9999 - val_loss: 0.5506 - val_acc: 0.9245\n",
      "Epoch 296/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1512 - acc: 0.9999 - val_loss: 0.5433 - val_acc: 0.9252\n",
      "Epoch 297/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1513 - acc: 0.9998 - val_loss: 0.5497 - val_acc: 0.9230\n",
      "Epoch 298/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1512 - acc: 0.9999 - val_loss: 0.5465 - val_acc: 0.9254\n",
      "Epoch 299/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1511 - acc: 0.9999 - val_loss: 0.5559 - val_acc: 0.9225\n",
      "Epoch 300/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1512 - acc: 0.9998 - val_loss: 0.5490 - val_acc: 0.9241\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# set callback\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# start training\n",
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('mobilenet_slim_n_0.75.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
