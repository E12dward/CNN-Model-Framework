{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D,GlobalAveragePooling2D, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import DepthwiseConv2D\n",
    "\n",
    "from keras import optimizers,regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "\n",
    "num_classes        = 10\n",
    "batch_size         = 64         # 64 or 32 or other\n",
    "epochs             = 300\n",
    "iterations         = 782       \n",
    "USE_BN=True\n",
    "DROPOUT=0.2 # keep 80%\n",
    "CONCAT_AXIS=3\n",
    "weight_decay=1e-4\n",
    "DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'\n",
    "\n",
    "log_filepath  = './mobilenet_n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.01\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    return 0.0001\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depthwise_separable(x,params):\n",
    "    # f1/f2 filter size, s1 stride of conv\n",
    "    '''\n",
    "    (s1,f2) = params\n",
    "    x = DepthwiseConv2D((3,3),strides=(s1[0],s1[0]), padding='same',depthwise_initializer=\"he_normal\")(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(int(f2[0]), (1,1), strides=(1,1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    '''\n",
    "    (s1,f2)=params\n",
    "    x=DepthwiseConv2D((3,3),strides=(s1[0],s1[0]),padding='same',depthwise_initializer='he_normal')(x)\n",
    "    x=BatchNormalization(momentum=0.9,epsilon=1e-5)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    x=Conv2D(int(f2[0]),(1,1),strides=(1,1),padding='same',kernel_initializer='he_normal',\n",
    "             kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x=BatchNormalization(momentum=0.9,epsilon=1e-5)(x)\n",
    "    x=Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MobileNet(img_input,shallow=False, classes=10):\n",
    "    \"\"\"Instantiates the MobileNet.Network has two hyper-parameters\n",
    "        which are the width of network (controlled by alpha)\n",
    "        and input size.\n",
    "        # Arguments\n",
    "            alpha: optional parameter of the network to change the \n",
    "                width of model.\n",
    "            shallow: optional parameter for making network smaller.\n",
    "            classes: optional number of classes to classify images\n",
    "                into.\n",
    "    \"\"\"\n",
    "    x = Conv2D(int(32), (3,3), strides=(2,2), padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = depthwise_separable(x,params=[(1,),(64,)])\n",
    "    x = depthwise_separable(x,params=[(2,),(128,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(128,)])\n",
    "    x = depthwise_separable(x,params=[(2,),(256,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(256,)])\n",
    "    x = depthwise_separable(x,params=[(2,),(512,)])\n",
    "    if not shallow:\n",
    "        for _ in range(5):\n",
    "            x = depthwise_separable(x,params=[(1,),(512,)])\n",
    "            \n",
    "    x = depthwise_separable(x,params=[(2,),(1024,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(1024,)])\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    out = Dense(classes, activation='softmax')(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 16, 16, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 8, 8, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 8, 8, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 4, 4, 128)         1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 4, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 4, 4, 256)         2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 256)         65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 2, 2, 256)         2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 2, 2, 512)         131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_7 (Depthwis (None, 2, 2, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 512)         262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_8 (Depthwis (None, 2, 2, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 2, 2, 512)         262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_9 (Depthwis (None, 2, 2, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 2, 512)         262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_10 (Depthwi (None, 2, 2, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 512)         262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_11 (Depthwi (None, 2, 2, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 512)         262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_12 (Depthwi (None, 1, 1, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_13 (Depthwi (None, 1, 1, 1024)        10240     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,250,058\n",
      "Trainable params: 3,228,170\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input=Input(shape=(32,32,3))\n",
    "output = MobileNet(img_input)\n",
    "model=Model(img_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 2.9389 - acc: 0.2943 - val_loss: 2.6469 - val_acc: 0.3918\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 46s 59ms/step - loss: 2.6100 - acc: 0.3984 - val_loss: 2.4728 - val_acc: 0.4355\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 46s 59ms/step - loss: 2.4676 - acc: 0.4403 - val_loss: 2.3028 - val_acc: 0.4897\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 2.3823 - acc: 0.4685 - val_loss: 2.2578 - val_acc: 0.4977\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 46s 58ms/step - loss: 2.3207 - acc: 0.4884 - val_loss: 2.2103 - val_acc: 0.5087\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 46s 58ms/step - loss: 2.2445 - acc: 0.5094 - val_loss: 2.1421 - val_acc: 0.5376\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 44s 56ms/step - loss: 2.1730 - acc: 0.5256 - val_loss: 2.0495 - val_acc: 0.5598\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 2.1442 - acc: 0.5358 - val_loss: 2.2805 - val_acc: 0.5497\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 41s 53ms/step - loss: 2.0979 - acc: 0.5450 - val_loss: 1.9310 - val_acc: 0.5958\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.9629 - acc: 0.5782 - val_loss: 1.8591 - val_acc: 0.6142\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.9023 - acc: 0.5928 - val_loss: 1.8663 - val_acc: 0.6169\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.8307 - acc: 0.6119 - val_loss: 1.7526 - val_acc: 0.6396\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.7720 - acc: 0.6299 - val_loss: 1.7448 - val_acc: 0.6408\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.7345 - acc: 0.6351 - val_loss: 1.6467 - val_acc: 0.6675\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.6866 - acc: 0.6490 - val_loss: 1.6217 - val_acc: 0.6679\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.6364 - acc: 0.6579 - val_loss: 1.5746 - val_acc: 0.6788\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.5890 - acc: 0.6708 - val_loss: 1.5266 - val_acc: 0.6914\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.5619 - acc: 0.6764 - val_loss: 1.5021 - val_acc: 0.6951\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.5194 - acc: 0.6862 - val_loss: 1.6161 - val_acc: 0.6588\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.5549 - acc: 0.6721 - val_loss: 1.4618 - val_acc: 0.6994\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.4882 - acc: 0.6880 - val_loss: 1.4563 - val_acc: 0.6949\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.4289 - acc: 0.7043 - val_loss: 1.3757 - val_acc: 0.7219\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.4329 - acc: 0.7023 - val_loss: 1.3737 - val_acc: 0.7148\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.3593 - acc: 0.7223 - val_loss: 1.3185 - val_acc: 0.7302\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.3158 - acc: 0.7306 - val_loss: 1.3104 - val_acc: 0.7307\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 1.2937 - acc: 0.7338 - val_loss: 1.2769 - val_acc: 0.7389\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.2712 - acc: 0.7410 - val_loss: 1.2564 - val_acc: 0.7362\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 41s 52ms/step - loss: 1.2441 - acc: 0.7432 - val_loss: 1.3075 - val_acc: 0.7201\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.2198 - acc: 0.7496 - val_loss: 1.2206 - val_acc: 0.7483\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.1970 - acc: 0.7530 - val_loss: 1.1833 - val_acc: 0.7607\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.1775 - acc: 0.7573 - val_loss: 1.2764 - val_acc: 0.7280\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.1580 - acc: 0.7622 - val_loss: 1.1625 - val_acc: 0.7587\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.1292 - acc: 0.7668 - val_loss: 1.1425 - val_acc: 0.7625\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.1665 - acc: 0.7535 - val_loss: 1.1653 - val_acc: 0.7566\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.1227 - acc: 0.7673 - val_loss: 1.1197 - val_acc: 0.7670\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.1001 - acc: 0.7714 - val_loss: 1.1891 - val_acc: 0.7451\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.0828 - acc: 0.7737 - val_loss: 1.0734 - val_acc: 0.7774\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.0405 - acc: 0.7857 - val_loss: 1.0687 - val_acc: 0.7760\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.0257 - acc: 0.7873 - val_loss: 1.0818 - val_acc: 0.7728\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.0169 - acc: 0.7884 - val_loss: 1.0522 - val_acc: 0.7783\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 1.0099 - acc: 0.7905 - val_loss: 1.0766 - val_acc: 0.7734oss: 1.0093 \n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.9933 - acc: 0.7914 - val_loss: 1.0515 - val_acc: 0.7711\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.9781 - acc: 0.7932 - val_loss: 1.0090 - val_acc: 0.7867\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.9697 - acc: 0.7971 - val_loss: 1.0447 - val_acc: 0.7736\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.9423 - acc: 0.8045 - val_loss: 1.0058 - val_acc: 0.7874\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.9221 - acc: 0.8075 - val_loss: 1.0202 - val_acc: 0.7798\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.9162 - acc: 0.8091 - val_loss: 0.9974 - val_acc: 0.7841\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.9035 - acc: 0.8117 - val_loss: 0.9952 - val_acc: 0.7813\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8886 - acc: 0.8156 - val_loss: 0.9908 - val_acc: 0.7936\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8859 - acc: 0.8115 - val_loss: 1.0184 - val_acc: 0.7748\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.8720 - acc: 0.8178 - val_loss: 0.9874 - val_acc: 0.7819\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8512 - acc: 0.8229 - val_loss: 0.9579 - val_acc: 0.7930\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.8462 - acc: 0.8253 - val_loss: 0.9364 - val_acc: 0.7925\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8404 - acc: 0.8232 - val_loss: 0.9474 - val_acc: 0.7899\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.8283 - acc: 0.8273 - val_loss: 0.9578 - val_acc: 0.7874\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8137 - acc: 0.8305 - val_loss: 0.9296 - val_acc: 0.7909\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8124 - acc: 0.8284 - val_loss: 0.9193 - val_acc: 0.7961\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.8029 - acc: 0.8331 - val_loss: 0.9675 - val_acc: 0.7802\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 41s 52ms/step - loss: 0.7916 - acc: 0.8345 - val_loss: 0.8959 - val_acc: 0.8052\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7814 - acc: 0.8366 - val_loss: 0.8951 - val_acc: 0.8009\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.7766 - acc: 0.8376 - val_loss: 0.8945 - val_acc: 0.8059\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7804 - acc: 0.8368 - val_loss: 0.9134 - val_acc: 0.7956\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7833 - acc: 0.8344 - val_loss: 0.8943 - val_acc: 0.8039\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.7623 - acc: 0.8411 - val_loss: 0.8747 - val_acc: 0.8089\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7630 - acc: 0.8402 - val_loss: 0.8826 - val_acc: 0.8074\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7462 - acc: 0.8433 - val_loss: 0.8839 - val_acc: 0.8056\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7397 - acc: 0.8447 - val_loss: 0.8886 - val_acc: 0.8050\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7226 - acc: 0.8495 - val_loss: 0.8589 - val_acc: 0.8109\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7335 - acc: 0.8462 - val_loss: 0.8721 - val_acc: 0.8088\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7259 - acc: 0.8492 - val_loss: 0.8634 - val_acc: 0.8065\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.7118 - acc: 0.8516 - val_loss: 0.8734 - val_acc: 0.8102\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.7127 - acc: 0.8522 - val_loss: 0.8871 - val_acc: 0.7980\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6911 - acc: 0.8587 - val_loss: 0.8780 - val_acc: 0.8076\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6966 - acc: 0.8557 - val_loss: 0.8488 - val_acc: 0.8167\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.6845 - acc: 0.8598 - val_loss: 0.8634 - val_acc: 0.8112\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6845 - acc: 0.8598 - val_loss: 0.8530 - val_acc: 0.8121\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6812 - acc: 0.8593 - val_loss: 0.8790 - val_acc: 0.8034\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6672 - acc: 0.8637 - val_loss: 0.8511 - val_acc: 0.8166\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.6726 - acc: 0.8617 - val_loss: 0.8673 - val_acc: 0.8136\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6654 - acc: 0.8640 - val_loss: 0.8493 - val_acc: 0.8095\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6572 - acc: 0.8647 - val_loss: 0.8733 - val_acc: 0.8040\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.6662 - acc: 0.8626 - val_loss: 0.8270 - val_acc: 0.8193\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.6505 - acc: 0.8674 - val_loss: 0.8903 - val_acc: 0.8106\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.6425 - acc: 0.8715 - val_loss: 0.8392 - val_acc: 0.8139\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6354 - acc: 0.8726 - val_loss: 0.8294 - val_acc: 0.8173\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.6334 - acc: 0.8709 - val_loss: 0.8385 - val_acc: 0.8145\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6292 - acc: 0.8743 - val_loss: 1.0075 - val_acc: 0.8000\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6353 - acc: 0.8697 - val_loss: 0.8430 - val_acc: 0.8161\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.6337 - acc: 0.8721 - val_loss: 0.8845 - val_acc: 0.8110\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6234 - acc: 0.8744 - val_loss: 0.8235 - val_acc: 0.8174\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.6146 - acc: 0.8791 - val_loss: 0.8519 - val_acc: 0.8165\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6141 - acc: 0.8773 - val_loss: 0.8221 - val_acc: 0.8261\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6041 - acc: 0.8817 - val_loss: 0.8316 - val_acc: 0.8200\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6076 - acc: 0.8815 - val_loss: 0.8223 - val_acc: 0.8248\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6010 - acc: 0.8830 - val_loss: 0.8381 - val_acc: 0.8217\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6016 - acc: 0.8812 - val_loss: 0.8481 - val_acc: 0.8117\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.6034 - acc: 0.8814 - val_loss: 0.8108 - val_acc: 0.8220\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.5935 - acc: 0.8841 - val_loss: 0.8499 - val_acc: 0.8141\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.5912 - acc: 0.8868 - val_loss: 0.8277 - val_acc: 0.8231\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.5958 - acc: 0.8851 - val_loss: 0.8202 - val_acc: 0.8196\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.5258 - acc: 0.9091 - val_loss: 0.7974 - val_acc: 0.8377\n",
      "Epoch 102/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4884 - acc: 0.9219 - val_loss: 0.8009 - val_acc: 0.8388\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4729 - acc: 0.9259 - val_loss: 0.8062 - val_acc: 0.8398\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4611 - acc: 0.9290 - val_loss: 0.8188 - val_acc: 0.8363\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4521 - acc: 0.9345 - val_loss: 0.8160 - val_acc: 0.8393\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4445 - acc: 0.9354 - val_loss: 0.8217 - val_acc: 0.8405\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4350 - acc: 0.9382 - val_loss: 0.8358 - val_acc: 0.8378\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.4276 - acc: 0.9408 - val_loss: 0.8334 - val_acc: 0.8386\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4226 - acc: 0.9436 - val_loss: 0.8262 - val_acc: 0.8408\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.4188 - acc: 0.9441 - val_loss: 0.8448 - val_acc: 0.8394\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4125 - acc: 0.9462 - val_loss: 0.8495 - val_acc: 0.8401\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4099 - acc: 0.9462 - val_loss: 0.8579 - val_acc: 0.8405\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4039 - acc: 0.9474 - val_loss: 0.8602 - val_acc: 0.8367\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.4016 - acc: 0.9479 - val_loss: 0.8670 - val_acc: 0.8392\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.3986 - acc: 0.9504 - val_loss: 0.8682 - val_acc: 0.8425\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3980 - acc: 0.9493 - val_loss: 0.8648 - val_acc: 0.8379\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3909 - acc: 0.9516 - val_loss: 0.8717 - val_acc: 0.8383\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3854 - acc: 0.9536 - val_loss: 0.8616 - val_acc: 0.8395\n",
      "Epoch 119/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3865 - acc: 0.9521 - val_loss: 0.8727 - val_acc: 0.8403\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3771 - acc: 0.9557 - val_loss: 0.8815 - val_acc: 0.8392\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 40s 51ms/step - loss: 0.3808 - acc: 0.9543 - val_loss: 0.8724 - val_acc: 0.8399\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 41s 52ms/step - loss: 0.3753 - acc: 0.9568 - val_loss: 0.8725 - val_acc: 0.8394\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3727 - acc: 0.9564 - val_loss: 0.8790 - val_acc: 0.8414\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3695 - acc: 0.9586 - val_loss: 0.8957 - val_acc: 0.8386\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3742 - acc: 0.9561 - val_loss: 0.8868 - val_acc: 0.8410\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3678 - acc: 0.9586 - val_loss: 0.8910 - val_acc: 0.8439\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3665 - acc: 0.9578 - val_loss: 0.9031 - val_acc: 0.8400\n",
      "Epoch 128/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3593 - acc: 0.9604 - val_loss: 0.9063 - val_acc: 0.8346\n",
      "Epoch 129/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.3629 - acc: 0.9593 - val_loss: 0.8987 - val_acc: 0.8390\n",
      "Epoch 130/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3542 - acc: 0.9615 - val_loss: 0.9030 - val_acc: 0.8378\n",
      "Epoch 131/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3501 - acc: 0.9624 - val_loss: 0.9046 - val_acc: 0.8418\n",
      "Epoch 132/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3508 - acc: 0.9628 - val_loss: 0.9079 - val_acc: 0.8411\n",
      "Epoch 133/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3494 - acc: 0.9630 - val_loss: 0.9233 - val_acc: 0.8390\n",
      "Epoch 134/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3435 - acc: 0.9651 - val_loss: 0.9448 - val_acc: 0.8379\n",
      "Epoch 135/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3431 - acc: 0.9647 - val_loss: 0.9195 - val_acc: 0.8390\n",
      "Epoch 136/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3433 - acc: 0.9647 - val_loss: 0.9215 - val_acc: 0.8385\n",
      "Epoch 137/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3409 - acc: 0.9657 - val_loss: 0.9413 - val_acc: 0.8364\n",
      "Epoch 138/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3379 - acc: 0.9659 - val_loss: 0.9225 - val_acc: 0.8400\n",
      "Epoch 139/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3355 - acc: 0.9660 - val_loss: 0.9518 - val_acc: 0.8348\n",
      "Epoch 140/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3366 - acc: 0.9657 - val_loss: 0.9519 - val_acc: 0.8370\n",
      "Epoch 141/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3302 - acc: 0.9693 - val_loss: 0.9464 - val_acc: 0.8396\n",
      "Epoch 142/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3315 - acc: 0.9674 - val_loss: 0.9469 - val_acc: 0.8388\n",
      "Epoch 143/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3316 - acc: 0.9674 - val_loss: 0.9359 - val_acc: 0.8390\n",
      "Epoch 144/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3275 - acc: 0.9683 - val_loss: 0.9524 - val_acc: 0.8372\n",
      "Epoch 145/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3265 - acc: 0.9687 - val_loss: 0.9582 - val_acc: 0.8392\n",
      "Epoch 146/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3237 - acc: 0.9690 - val_loss: 0.9652 - val_acc: 0.8358\n",
      "Epoch 147/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3239 - acc: 0.9687 - val_loss: 0.9551 - val_acc: 0.8376\n",
      "Epoch 148/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.3215 - acc: 0.9696 - val_loss: 0.9557 - val_acc: 0.8354\n",
      "Epoch 149/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3227 - acc: 0.9688 - val_loss: 0.9520 - val_acc: 0.8358\n",
      "Epoch 150/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3169 - acc: 0.9702 - val_loss: 0.9718 - val_acc: 0.8375\n",
      "Epoch 151/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3181 - acc: 0.9704 - val_loss: 0.9656 - val_acc: 0.8335\n",
      "Epoch 152/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3196 - acc: 0.9700 - val_loss: 0.9568 - val_acc: 0.8369\n",
      "Epoch 153/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3150 - acc: 0.9704 - val_loss: 0.9626 - val_acc: 0.8365\n",
      "Epoch 154/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.3113 - acc: 0.9717 - val_loss: 0.9588 - val_acc: 0.8391\n",
      "Epoch 155/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.3115 - acc: 0.9711 - val_loss: 0.9782 - val_acc: 0.8344\n",
      "Epoch 156/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3125 - acc: 0.9710 - val_loss: 0.9640 - val_acc: 0.8364\n",
      "Epoch 157/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3094 - acc: 0.9724 - val_loss: 0.9623 - val_acc: 0.8386\n",
      "Epoch 158/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.3084 - acc: 0.9724 - val_loss: 0.9735 - val_acc: 0.8368\n",
      "Epoch 159/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3072 - acc: 0.9719 - val_loss: 0.9636 - val_acc: 0.8357\n",
      "Epoch 160/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3048 - acc: 0.9733 - val_loss: 0.9692 - val_acc: 0.8358\n",
      "Epoch 161/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.3031 - acc: 0.9737 - val_loss: 0.9807 - val_acc: 0.8383\n",
      "Epoch 162/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3045 - acc: 0.9722 - val_loss: 0.9788 - val_acc: 0.8334\n",
      "Epoch 163/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.3044 - acc: 0.9726 - val_loss: 0.9704 - val_acc: 0.8376\n",
      "Epoch 164/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.3005 - acc: 0.9736 - val_loss: 0.9721 - val_acc: 0.8386\n",
      "Epoch 165/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.3024 - acc: 0.9735 - val_loss: 0.9778 - val_acc: 0.8370\n",
      "Epoch 166/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2995 - acc: 0.9727 - val_loss: 1.0136 - val_acc: 0.8328\n",
      "Epoch 167/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2964 - acc: 0.9752 - val_loss: 0.9877 - val_acc: 0.8357\n",
      "Epoch 168/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2957 - acc: 0.9752 - val_loss: 0.9954 - val_acc: 0.8378\n",
      "Epoch 169/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.2961 - acc: 0.9733 - val_loss: 0.9979 - val_acc: 0.8347\n",
      "Epoch 170/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2916 - acc: 0.9755 - val_loss: 1.0145 - val_acc: 0.8379\n",
      "Epoch 171/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2951 - acc: 0.9752 - val_loss: 0.9873 - val_acc: 0.8379\n",
      "Epoch 172/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2928 - acc: 0.9746 - val_loss: 0.9903 - val_acc: 0.8382\n",
      "Epoch 173/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2922 - acc: 0.9752 - val_loss: 0.9743 - val_acc: 0.8383\n",
      "Epoch 174/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2899 - acc: 0.9754 - val_loss: 0.9776 - val_acc: 0.8422\n",
      "Epoch 175/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2886 - acc: 0.9762 - val_loss: 0.9870 - val_acc: 0.8374\n",
      "Epoch 176/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2847 - acc: 0.9765 - val_loss: 0.9967 - val_acc: 0.8393\n",
      "Epoch 177/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2872 - acc: 0.9762 - val_loss: 0.9940 - val_acc: 0.8374\n",
      "Epoch 178/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2852 - acc: 0.9763 - val_loss: 0.9775 - val_acc: 0.8418\n",
      "Epoch 179/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2849 - acc: 0.9757 - val_loss: 1.0057 - val_acc: 0.8377\n",
      "Epoch 180/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2821 - acc: 0.9776 - val_loss: 0.9814 - val_acc: 0.8408\n",
      "Epoch 181/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2793 - acc: 0.9777 - val_loss: 0.9982 - val_acc: 0.8392\n",
      "Epoch 182/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2813 - acc: 0.9771 - val_loss: 0.9999 - val_acc: 0.8374\n",
      "Epoch 183/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2803 - acc: 0.9773 - val_loss: 1.0224 - val_acc: 0.8325\n",
      "Epoch 184/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2763 - acc: 0.9786 - val_loss: 1.0196 - val_acc: 0.8351\n",
      "Epoch 185/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2786 - acc: 0.9771 - val_loss: 1.0074 - val_acc: 0.8343\n",
      "Epoch 186/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2764 - acc: 0.9785 - val_loss: 1.0029 - val_acc: 0.8396\n",
      "Epoch 187/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2777 - acc: 0.9774 - val_loss: 1.0147 - val_acc: 0.8373\n",
      "Epoch 188/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2781 - acc: 0.9763 - val_loss: 0.9904 - val_acc: 0.8393\n",
      "Epoch 189/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2760 - acc: 0.9778 - val_loss: 0.9842 - val_acc: 0.8375\n",
      "Epoch 190/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2736 - acc: 0.9787 - val_loss: 0.9998 - val_acc: 0.8397\n",
      "Epoch 191/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2752 - acc: 0.9777 - val_loss: 0.9853 - val_acc: 0.8413\n",
      "Epoch 192/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2738 - acc: 0.9777 - val_loss: 0.9783 - val_acc: 0.8377\n",
      "Epoch 193/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2695 - acc: 0.9798 - val_loss: 0.9934 - val_acc: 0.8364\n",
      "Epoch 194/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2720 - acc: 0.9792 - val_loss: 0.9847 - val_acc: 0.8386\n",
      "Epoch 195/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2689 - acc: 0.9796 - val_loss: 1.0079 - val_acc: 0.8356\n",
      "Epoch 196/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2691 - acc: 0.9787 - val_loss: 1.0103 - val_acc: 0.8347\n",
      "Epoch 197/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2680 - acc: 0.9801 - val_loss: 1.0249 - val_acc: 0.8344\n",
      "Epoch 198/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2686 - acc: 0.9783 - val_loss: 1.0042 - val_acc: 0.8398\n",
      "Epoch 199/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2669 - acc: 0.9795 - val_loss: 0.9989 - val_acc: 0.8370\n",
      "Epoch 200/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2651 - acc: 0.9800 - val_loss: 0.9994 - val_acc: 0.8365\n",
      "Epoch 201/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2575 - acc: 0.9827 - val_loss: 0.9942 - val_acc: 0.8387\n",
      "Epoch 202/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2552 - acc: 0.9835 - val_loss: 0.9930 - val_acc: 0.8369\n",
      "Epoch 203/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2522 - acc: 0.9846 - val_loss: 0.9900 - val_acc: 0.8400\n",
      "Epoch 204/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2520 - acc: 0.9844 - val_loss: 0.9943 - val_acc: 0.8392\n",
      "Epoch 205/300\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.2510 - acc: 0.9854 - val_loss: 1.0053 - val_acc: 0.8384\n",
      "Epoch 206/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2533 - acc: 0.9842 - val_loss: 0.9993 - val_acc: 0.8409\n",
      "Epoch 207/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2499 - acc: 0.9847 - val_loss: 0.9943 - val_acc: 0.8391\n",
      "Epoch 208/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2497 - acc: 0.9854 - val_loss: 0.9987 - val_acc: 0.8402\n",
      "Epoch 209/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2487 - acc: 0.9854 - val_loss: 1.0052 - val_acc: 0.8389\n",
      "Epoch 210/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2483 - acc: 0.9859 - val_loss: 1.0098 - val_acc: 0.8397\n",
      "Epoch 211/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2469 - acc: 0.9864 - val_loss: 1.0075 - val_acc: 0.8398\n",
      "Epoch 212/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2478 - acc: 0.9852 - val_loss: 1.0105 - val_acc: 0.8398\n",
      "Epoch 213/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2455 - acc: 0.9864 - val_loss: 1.0176 - val_acc: 0.8389\n",
      "Epoch 214/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2463 - acc: 0.9861 - val_loss: 1.0227 - val_acc: 0.8387\n",
      "Epoch 215/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2472 - acc: 0.9863 - val_loss: 1.0097 - val_acc: 0.8394\n",
      "Epoch 216/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2453 - acc: 0.9866 - val_loss: 1.0172 - val_acc: 0.8393\n",
      "Epoch 217/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2468 - acc: 0.9858 - val_loss: 1.0267 - val_acc: 0.8377\n",
      "Epoch 218/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2430 - acc: 0.9875 - val_loss: 1.0210 - val_acc: 0.8383\n",
      "Epoch 219/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2449 - acc: 0.9862 - val_loss: 1.0278 - val_acc: 0.8389\n",
      "Epoch 220/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2434 - acc: 0.9868 - val_loss: 1.0218 - val_acc: 0.8393\n",
      "Epoch 221/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.2430 - acc: 0.9872 - val_loss: 1.0171 - val_acc: 0.8406\n",
      "Epoch 222/300\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.2417 - acc: 0.9878 - val_loss: 1.0295 - val_acc: 0.8397\n",
      "Epoch 223/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2415 - acc: 0.9870 - val_loss: 1.0250 - val_acc: 0.8382\n",
      "Epoch 224/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2425 - acc: 0.9872 - val_loss: 1.0460 - val_acc: 0.8390\n",
      "Epoch 225/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2437 - acc: 0.9869 - val_loss: 1.0291 - val_acc: 0.8392\n",
      "Epoch 226/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2434 - acc: 0.9872 - val_loss: 1.0380 - val_acc: 0.8375\n",
      "Epoch 227/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2426 - acc: 0.9872 - val_loss: 1.0459 - val_acc: 0.8378\n",
      "Epoch 228/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2420 - acc: 0.9875 - val_loss: 1.0348 - val_acc: 0.8401\n",
      "Epoch 229/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2402 - acc: 0.9878 - val_loss: 1.0462 - val_acc: 0.8389\n",
      "Epoch 230/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2419 - acc: 0.9871 - val_loss: 1.0503 - val_acc: 0.8405\n",
      "Epoch 231/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2440 - acc: 0.9874 - val_loss: 1.0397 - val_acc: 0.8417\n",
      "Epoch 232/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2437 - acc: 0.9865 - val_loss: 1.0437 - val_acc: 0.8399\n",
      "Epoch 233/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2402 - acc: 0.9879 - val_loss: 1.0636 - val_acc: 0.8394\n",
      "Epoch 234/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2402 - acc: 0.9881 - val_loss: 1.0348 - val_acc: 0.8381\n",
      "Epoch 235/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2413 - acc: 0.9871 - val_loss: 1.0477 - val_acc: 0.8409\n",
      "Epoch 236/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2414 - acc: 0.9871 - val_loss: 1.0446 - val_acc: 0.8404\n",
      "Epoch 237/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2394 - acc: 0.9882 - val_loss: 1.0428 - val_acc: 0.8383\n",
      "Epoch 238/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2413 - acc: 0.9870 - val_loss: 1.0443 - val_acc: 0.8403\n",
      "Epoch 239/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2385 - acc: 0.9883 - val_loss: 1.0527 - val_acc: 0.8409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2392 - acc: 0.9881 - val_loss: 1.0477 - val_acc: 0.8404\n",
      "Epoch 241/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2384 - acc: 0.9878 - val_loss: 1.0575 - val_acc: 0.8390\n",
      "Epoch 242/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2390 - acc: 0.9883 - val_loss: 1.0665 - val_acc: 0.8389\n",
      "Epoch 243/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2382 - acc: 0.9882 - val_loss: 1.0735 - val_acc: 0.8387\n",
      "Epoch 244/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2363 - acc: 0.9888 - val_loss: 1.0512 - val_acc: 0.8415\n",
      "Epoch 245/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2386 - acc: 0.9883 - val_loss: 1.0658 - val_acc: 0.8393\n",
      "Epoch 246/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2378 - acc: 0.9883 - val_loss: 1.0488 - val_acc: 0.8423\n",
      "Epoch 247/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2380 - acc: 0.9881 - val_loss: 1.0697 - val_acc: 0.8400\n",
      "Epoch 248/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2369 - acc: 0.9886 - val_loss: 1.0618 - val_acc: 0.8420\n",
      "Epoch 249/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2380 - acc: 0.9879 - val_loss: 1.0743 - val_acc: 0.8430\n",
      "Epoch 250/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2386 - acc: 0.9880 - val_loss: 1.0654 - val_acc: 0.8418\n",
      "Epoch 251/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2378 - acc: 0.9878 - val_loss: 1.0618 - val_acc: 0.8421\n",
      "Epoch 252/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2386 - acc: 0.9887 - val_loss: 1.0715 - val_acc: 0.8397\n",
      "Epoch 253/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2382 - acc: 0.9882 - val_loss: 1.0574 - val_acc: 0.8403\n",
      "Epoch 254/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2364 - acc: 0.9890 - val_loss: 1.0535 - val_acc: 0.8390\n",
      "Epoch 255/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2367 - acc: 0.9886 - val_loss: 1.0588 - val_acc: 0.8408\n",
      "Epoch 256/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2373 - acc: 0.9882 - val_loss: 1.0711 - val_acc: 0.8395\n",
      "Epoch 257/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2380 - acc: 0.9881 - val_loss: 1.0524 - val_acc: 0.8402\n",
      "Epoch 258/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2362 - acc: 0.9896 - val_loss: 1.0643 - val_acc: 0.8391\n",
      "Epoch 259/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2360 - acc: 0.9887 - val_loss: 1.0707 - val_acc: 0.8390\n",
      "Epoch 260/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2346 - acc: 0.9893 - val_loss: 1.0650 - val_acc: 0.8396\n",
      "Epoch 261/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2339 - acc: 0.9894 - val_loss: 1.0656 - val_acc: 0.8387\n",
      "Epoch 262/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2360 - acc: 0.9886 - val_loss: 1.0677 - val_acc: 0.8397\n",
      "Epoch 263/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2348 - acc: 0.9893 - val_loss: 1.0698 - val_acc: 0.8419\n",
      "Epoch 264/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2352 - acc: 0.9891 - val_loss: 1.0618 - val_acc: 0.8390\n",
      "Epoch 265/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2345 - acc: 0.9890 - val_loss: 1.0534 - val_acc: 0.8422\n",
      "Epoch 266/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2361 - acc: 0.9888 - val_loss: 1.0542 - val_acc: 0.8403\n",
      "Epoch 267/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2338 - acc: 0.9895 - val_loss: 1.0557 - val_acc: 0.8399\n",
      "Epoch 268/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2354 - acc: 0.9892 - val_loss: 1.0607 - val_acc: 0.8398\n",
      "Epoch 269/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.2340 - acc: 0.9896 - val_loss: 1.0656 - val_acc: 0.8404\n",
      "Epoch 270/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2372 - acc: 0.9891 - val_loss: 1.0706 - val_acc: 0.8393\n",
      "Epoch 271/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2324 - acc: 0.9896 - val_loss: 1.0850 - val_acc: 0.8416\n",
      "Epoch 272/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2336 - acc: 0.9890 - val_loss: 1.0645 - val_acc: 0.8407\n",
      "Epoch 273/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2347 - acc: 0.9890 - val_loss: 1.0560 - val_acc: 0.8415\n",
      "Epoch 274/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2338 - acc: 0.9894 - val_loss: 1.0851 - val_acc: 0.8402\n",
      "Epoch 275/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2353 - acc: 0.9890 - val_loss: 1.0845 - val_acc: 0.8405\n",
      "Epoch 276/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2326 - acc: 0.9896 - val_loss: 1.0757 - val_acc: 0.8394\n",
      "Epoch 277/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2337 - acc: 0.9890 - val_loss: 1.0734 - val_acc: 0.8410\n",
      "Epoch 278/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2334 - acc: 0.9894 - val_loss: 1.0779 - val_acc: 0.8425\n",
      "Epoch 279/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2330 - acc: 0.9895 - val_loss: 1.0824 - val_acc: 0.8406\n",
      "Epoch 280/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2321 - acc: 0.9898 - val_loss: 1.0848 - val_acc: 0.8408\n",
      "Epoch 281/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2317 - acc: 0.9897 - val_loss: 1.0734 - val_acc: 0.8408\n",
      "Epoch 282/300\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.2330 - acc: 0.9900 - val_loss: 1.0774 - val_acc: 0.8426\n",
      "Epoch 283/300\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.2317 - acc: 0.9892 - val_loss: 1.0692 - val_acc: 0.8425\n",
      "Epoch 284/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2337 - acc: 0.9894 - val_loss: 1.0737 - val_acc: 0.8408\n",
      "Epoch 285/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2303 - acc: 0.9902 - val_loss: 1.0806 - val_acc: 0.8412\n",
      "Epoch 286/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2333 - acc: 0.9892 - val_loss: 1.0751 - val_acc: 0.8408\n",
      "Epoch 287/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2315 - acc: 0.9896 - val_loss: 1.0746 - val_acc: 0.8410\n",
      "Epoch 288/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2326 - acc: 0.9897 - val_loss: 1.0713 - val_acc: 0.8404\n",
      "Epoch 289/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2306 - acc: 0.9900 - val_loss: 1.0745 - val_acc: 0.8406\n",
      "Epoch 290/300\n",
      "782/782 [==============================] - 39s 51ms/step - loss: 0.2308 - acc: 0.9899 - val_loss: 1.0749 - val_acc: 0.8415\n",
      "Epoch 291/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.2313 - acc: 0.9898 - val_loss: 1.0693 - val_acc: 0.8421\n",
      "Epoch 292/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.2306 - acc: 0.9893 - val_loss: 1.0882 - val_acc: 0.8402\n",
      "Epoch 293/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2292 - acc: 0.9902 - val_loss: 1.0870 - val_acc: 0.8407\n",
      "Epoch 294/300\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.2305 - acc: 0.9899 - val_loss: 1.0750 - val_acc: 0.8426\n",
      "Epoch 295/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2321 - acc: 0.9894 - val_loss: 1.0788 - val_acc: 0.8412\n",
      "Epoch 296/300\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.2283 - acc: 0.9906 - val_loss: 1.0811 - val_acc: 0.8411\n",
      "Epoch 297/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2291 - acc: 0.9903 - val_loss: 1.0847 - val_acc: 0.8415\n",
      "Epoch 298/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2299 - acc: 0.9898 - val_loss: 1.1054 - val_acc: 0.8384\n",
      "Epoch 299/300\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 0.2299 - acc: 0.9902 - val_loss: 1.0857 - val_acc: 0.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.2303 - acc: 0.9905 - val_loss: 1.0803 - val_acc: 0.8403\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# set callback\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# start training\n",
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('mobilenet_n.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
