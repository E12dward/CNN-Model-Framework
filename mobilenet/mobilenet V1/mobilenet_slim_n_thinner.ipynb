{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" \n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D,GlobalAveragePooling2D, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import DepthwiseConv2D\n",
    "\n",
    "from keras import optimizers,regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "\n",
    "num_classes        = 10\n",
    "batch_size         = 64         # 64 or 32 or other\n",
    "epochs             = 300\n",
    "iterations         = 782       \n",
    "USE_BN=True\n",
    "DROPOUT=0.2 # keep 80%\n",
    "CONCAT_AXIS=3\n",
    "weight_decay=1e-4\n",
    "DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'\n",
    "\n",
    "log_filepath  = './mobilenet_slim_n_thinner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.01\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    return 0.0001\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depthwise_separable(x,params):\n",
    "    # f1/f2 filter size, s1 stride of conv\n",
    "    (s1,f2) = params\n",
    "    x = DepthwiseConv2D((3,3),strides=(s1[0],s1[0]), padding='same',\n",
    "                        depthwise_initializer=\"he_normal\",depthwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(int(f2[0]), (1,1), strides=(1,1), padding='same',\n",
    "               kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MobileNet(img_input,shallow=True, classes=10):\n",
    "    \"\"\"Instantiates the MobileNet.Network has two hyper-parameters\n",
    "        which are the width of network (controlled by alpha)\n",
    "        and input size.\n",
    "        # Arguments\n",
    "            alpha: optional parameter of the network to change the \n",
    "                width of model.\n",
    "            shallow: optional parameter for making network smaller.\n",
    "            classes: optional number of classes to classify images\n",
    "                into.\n",
    "    \"\"\"\n",
    "    # change stride\n",
    "    x = Conv2D(int(32), (3,3), strides=(1,1), padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = depthwise_separable(x,params=[(1,),(64,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(128,)])# change stride\n",
    "    x = depthwise_separable(x,params=[(1,),(128,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(256,)])# change stride\n",
    "    x = depthwise_separable(x,params=[(1,),(256,)])\n",
    "    x = depthwise_separable(x,params=[(2,),(512,)])\n",
    "    if not shallow:\n",
    "        for _ in range(5):\n",
    "            x = depthwise_separable(x,params=[(1,),(512,)])\n",
    "    x = depthwise_separable(x,params=[(2,),(1024,)])\n",
    "    x = depthwise_separable(x,params=[(1,),(1024,)])\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    out = Dense(classes, activation='softmax')(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_2 (Depthwis (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_3 (Depthwis (None, 32, 32, 128)       1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 128)       16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_4 (Depthwis (None, 32, 32, 128)       1280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 256)       33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_5 (Depthwis (None, 32, 32, 256)       2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_6 (Depthwis (None, 16, 16, 256)       2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 512)       131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_7 (Depthwis (None, 8, 8, 512)         5120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_8 (Depthwis (None, 8, 8, 1024)        10240     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 8, 8, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 8, 8, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,890,698\n",
      "Trainable params: 1,879,050\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input=Input(shape=(32,32,3))\n",
    "output = MobileNet(img_input)\n",
    "model=Model(img_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 64s 82ms/step - loss: 2.0718 - acc: 0.4300 - val_loss: 2.0029 - val_acc: 0.4881\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 1.6738 - acc: 0.5875 - val_loss: 1.7229 - val_acc: 0.5865\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 1.4667 - acc: 0.6603 - val_loss: 1.4727 - val_acc: 0.6620\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 1.3204 - acc: 0.7097 - val_loss: 1.2991 - val_acc: 0.7192\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 1.2324 - acc: 0.7397 - val_loss: 1.2108 - val_acc: 0.7528\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 1.1514 - acc: 0.7660 - val_loss: 1.1870 - val_acc: 0.7613\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 1.0853 - acc: 0.7845 - val_loss: 1.2112 - val_acc: 0.7501\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 1.0366 - acc: 0.7990 - val_loss: 1.1762 - val_acc: 0.7657\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 0.9884 - acc: 0.8114 - val_loss: 1.0293 - val_acc: 0.8035\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.9460 - acc: 0.8245 - val_loss: 1.1051 - val_acc: 0.7824loss: 0.9464 \n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 0.9097 - acc: 0.8332 - val_loss: 1.1795 - val_acc: 0.7612\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.8752 - acc: 0.8439 - val_loss: 0.9404 - val_acc: 0.8239\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.8409 - acc: 0.8521 - val_loss: 0.9499 - val_acc: 0.8226\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.8185 - acc: 0.8569 - val_loss: 0.9536 - val_acc: 0.8235\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.7899 - acc: 0.8630 - val_loss: 0.9203 - val_acc: 0.8266\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.7622 - acc: 0.8700 - val_loss: 0.9210 - val_acc: 0.8306\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.7372 - acc: 0.8765 - val_loss: 0.9252 - val_acc: 0.8291\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.7194 - acc: 0.8798 - val_loss: 0.9383 - val_acc: 0.8209\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.6998 - acc: 0.8838 - val_loss: 0.8900 - val_acc: 0.8336\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.6748 - acc: 0.8903 - val_loss: 0.8562 - val_acc: 0.8483\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.6607 - acc: 0.8940 - val_loss: 0.8774 - val_acc: 0.8368\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.6378 - acc: 0.8982 - val_loss: 0.8008 - val_acc: 0.8543\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.6265 - acc: 0.9010 - val_loss: 0.8650 - val_acc: 0.8412\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.6061 - acc: 0.9076 - val_loss: 0.8438 - val_acc: 0.8471\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.5942 - acc: 0.9081 - val_loss: 0.7570 - val_acc: 0.8678\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.5752 - acc: 0.9133 - val_loss: 0.8271 - val_acc: 0.8500\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.5633 - acc: 0.9153 - val_loss: 0.7833 - val_acc: 0.8583\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.5502 - acc: 0.9172 - val_loss: 0.8181 - val_acc: 0.8539\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.5427 - acc: 0.9179 - val_loss: 0.7443 - val_acc: 0.8654\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.5299 - acc: 0.9217 - val_loss: 0.7713 - val_acc: 0.8599\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.5189 - acc: 0.9244 - val_loss: 0.9263 - val_acc: 0.8294\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.5048 - acc: 0.9280 - val_loss: 0.8435 - val_acc: 0.8420\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4982 - acc: 0.9270 - val_loss: 0.8185 - val_acc: 0.8522\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4857 - acc: 0.9315 - val_loss: 0.7457 - val_acc: 0.8692\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.4746 - acc: 0.9330 - val_loss: 0.7381 - val_acc: 0.8663\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.4692 - acc: 0.9337 - val_loss: 0.7368 - val_acc: 0.8707\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.4565 - acc: 0.9360 - val_loss: 0.7327 - val_acc: 0.8697\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4476 - acc: 0.9376 - val_loss: 0.8611 - val_acc: 0.8458\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.4426 - acc: 0.9402 - val_loss: 0.7907 - val_acc: 0.8599\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4345 - acc: 0.9410 - val_loss: 0.6866 - val_acc: 0.8772\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.4293 - acc: 0.9428 - val_loss: 0.7990 - val_acc: 0.8571\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.4210 - acc: 0.9439 - val_loss: 0.7073 - val_acc: 0.8752\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4122 - acc: 0.9467 - val_loss: 0.6925 - val_acc: 0.8746\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4039 - acc: 0.9466 - val_loss: 0.7190 - val_acc: 0.8703\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.4008 - acc: 0.9467 - val_loss: 0.7309 - val_acc: 0.8664\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3924 - acc: 0.9496 - val_loss: 0.7276 - val_acc: 0.8659\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3960 - acc: 0.9465 - val_loss: 0.7682 - val_acc: 0.8636\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.3845 - acc: 0.9500 - val_loss: 0.7321 - val_acc: 0.8696\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.3737 - acc: 0.9540 - val_loss: 0.7654 - val_acc: 0.8662\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3658 - acc: 0.9557 - val_loss: 0.6973 - val_acc: 0.8758\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3661 - acc: 0.9542 - val_loss: 0.6907 - val_acc: 0.8761\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.3633 - acc: 0.9550 - val_loss: 0.7275 - val_acc: 0.8699\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3631 - acc: 0.9541 - val_loss: 0.7145 - val_acc: 0.8707\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.3604 - acc: 0.9541 - val_loss: 0.8629 - val_acc: 0.8542\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3485 - acc: 0.9582 - val_loss: 0.6838 - val_acc: 0.8771\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.3518 - acc: 0.9559 - val_loss: 0.7052 - val_acc: 0.8793\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3409 - acc: 0.9600 - val_loss: 0.6630 - val_acc: 0.8810\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.3393 - acc: 0.9587 - val_loss: 0.6756 - val_acc: 0.8794\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3330 - acc: 0.9603 - val_loss: 0.7179 - val_acc: 0.8734\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.3304 - acc: 0.9599 - val_loss: 0.6395 - val_acc: 0.8858\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3287 - acc: 0.9613 - val_loss: 0.6550 - val_acc: 0.8853\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3261 - acc: 0.9595 - val_loss: 0.6695 - val_acc: 0.8796\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3220 - acc: 0.9623 - val_loss: 0.6340 - val_acc: 0.8903\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.3141 - acc: 0.9646 - val_loss: 0.6797 - val_acc: 0.8839\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3123 - acc: 0.9634 - val_loss: 0.7150 - val_acc: 0.8755\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.3135 - acc: 0.9639 - val_loss: 0.6869 - val_acc: 0.8802\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3129 - acc: 0.9638 - val_loss: 0.6577 - val_acc: 0.88730.3127 - acc:\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3041 - acc: 0.9658 - val_loss: 0.6728 - val_acc: 0.8867\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.3009 - acc: 0.9665 - val_loss: 0.6649 - val_acc: 0.8850\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.3028 - acc: 0.9654 - val_loss: 0.6623 - val_acc: 0.8835\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2995 - acc: 0.9662 - val_loss: 0.6554 - val_acc: 0.8858\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2951 - acc: 0.9671 - val_loss: 0.7229 - val_acc: 0.8732\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2903 - acc: 0.9686 - val_loss: 0.6833 - val_acc: 0.8835\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2939 - acc: 0.9671 - val_loss: 0.6966 - val_acc: 0.8775 loss: 0. - ETA: 2s - loss: 0.29 - ETA: 1s - loss: 0\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.2836 - acc: 0.9694 - val_loss: 0.6737 - val_acc: 0.8793\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2823 - acc: 0.9701 - val_loss: 0.7121 - val_acc: 0.8711\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2873 - acc: 0.9665 - val_loss: 0.7083 - val_acc: 0.8750 0.2863 -  - ETA: 2s - lo - ETA: 1s - loss: 0.\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2816 - acc: 0.9693 - val_loss: 0.6713 - val_acc: 0.8821\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2814 - acc: 0.9685 - val_loss: 0.7013 - val_acc: 0.8773- a - ETA: 1s - loss: 0\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2802 - acc: 0.9691 - val_loss: 0.6553 - val_acc: 0.8884\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2764 - acc: 0.9695 - val_loss: 0.6770 - val_acc: 0.8788\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2774 - acc: 0.9705 - val_loss: 0.6513 - val_acc: 0.8887\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2808 - acc: 0.9669 - val_loss: 0.6448 - val_acc: 0.8899\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2700 - acc: 0.9714 - val_loss: 0.6976 - val_acc: 0.8819\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.2716 - acc: 0.9708 - val_loss: 0.6244 - val_acc: 0.8948\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2633 - acc: 0.9740 - val_loss: 0.6390 - val_acc: 0.8893\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2698 - acc: 0.9706 - val_loss: 0.6578 - val_acc: 0.8901\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2638 - acc: 0.9730 - val_loss: 0.6699 - val_acc: 0.8855\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.2609 - acc: 0.9731 - val_loss: 0.6685 - val_acc: 0.8863\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2626 - acc: 0.9720 - val_loss: 0.6615 - val_acc: 0.8834\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2635 - acc: 0.9719 - val_loss: 0.6689 - val_acc: 0.8837\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2591 - acc: 0.9729 - val_loss: 0.6594 - val_acc: 0.8874\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.2536 - acc: 0.9742 - val_loss: 0.6081 - val_acc: 0.8926\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2571 - acc: 0.9727 - val_loss: 0.7104 - val_acc: 0.8775\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2575 - acc: 0.9728 - val_loss: 0.6181 - val_acc: 0.8930\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2596 - acc: 0.9716 - val_loss: 0.7144 - val_acc: 0.8780\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2554 - acc: 0.9724 - val_loss: 0.6659 - val_acc: 0.8871\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.2460 - acc: 0.9764 - val_loss: 0.8605 - val_acc: 0.8571ss\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.2448 - acc: 0.9754 - val_loss: 0.6362 - val_acc: 0.8908\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.2485 - acc: 0.9745 - val_loss: 0.6603 - val_acc: 0.8837 6s - \n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.2151 - acc: 0.9870 - val_loss: 0.5493 - val_acc: 0.9086ss: 0.2157 - acc: - ETA: 1s - loss\n",
      "Epoch 102/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1988 - acc: 0.9929 - val_loss: 0.5389 - val_acc: 0.9088\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1942 - acc: 0.9947 - val_loss: 0.5327 - val_acc: 0.9095\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1920 - acc: 0.9954 - val_loss: 0.5329 - val_acc: 0.9115\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1880 - acc: 0.9966 - val_loss: 0.5295 - val_acc: 0.9109\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1869 - acc: 0.9968 - val_loss: 0.5241 - val_acc: 0.9120\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1861 - acc: 0.9967 - val_loss: 0.5294 - val_acc: 0.9110\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1845 - acc: 0.9971 - val_loss: 0.5282 - val_acc: 0.9118\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1839 - acc: 0.9971 - val_loss: 0.5244 - val_acc: 0.9126\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1819 - acc: 0.9976 - val_loss: 0.5318 - val_acc: 0.9121\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1810 - acc: 0.9976 - val_loss: 0.5288 - val_acc: 0.9141\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1798 - acc: 0.9979 - val_loss: 0.5269 - val_acc: 0.9128\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1788 - acc: 0.9982 - val_loss: 0.5232 - val_acc: 0.9141\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1783 - acc: 0.9980 - val_loss: 0.5274 - val_acc: 0.9133\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1768 - acc: 0.9985 - val_loss: 0.5245 - val_acc: 0.9143\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1768 - acc: 0.9983 - val_loss: 0.5273 - val_acc: 0.9146\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1753 - acc: 0.9986 - val_loss: 0.5200 - val_acc: 0.9156\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1750 - acc: 0.9983 - val_loss: 0.5296 - val_acc: 0.9137\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1739 - acc: 0.9986 - val_loss: 0.5199 - val_acc: 0.9155\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1728 - acc: 0.9989 - val_loss: 0.5233 - val_acc: 0.9141\n",
      "Epoch 121/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1724 - acc: 0.9986 - val_loss: 0.5296 - val_acc: 0.9134\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1721 - acc: 0.9987 - val_loss: 0.5256 - val_acc: 0.9150\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1714 - acc: 0.9985 - val_loss: 0.5185 - val_acc: 0.9173\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1706 - acc: 0.9988 - val_loss: 0.5239 - val_acc: 0.9154\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1695 - acc: 0.9989 - val_loss: 0.5210 - val_acc: 0.9161\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1692 - acc: 0.9989 - val_loss: 0.5239 - val_acc: 0.9170\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1684 - acc: 0.9988 - val_loss: 0.5207 - val_acc: 0.9179\n",
      "Epoch 128/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1682 - acc: 0.9988 - val_loss: 0.5211 - val_acc: 0.9157\n",
      "Epoch 129/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1671 - acc: 0.9990 - val_loss: 0.5186 - val_acc: 0.9171\n",
      "Epoch 130/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1671 - acc: 0.9987 - val_loss: 0.5211 - val_acc: 0.9164 1s - loss: 0.1\n",
      "Epoch 131/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1661 - acc: 0.9990 - val_loss: 0.5316 - val_acc: 0.9140\n",
      "Epoch 132/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1652 - acc: 0.9989 - val_loss: 0.5260 - val_acc: 0.9153\n",
      "Epoch 133/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1650 - acc: 0.9990 - val_loss: 0.5242 - val_acc: 0.9156\n",
      "Epoch 134/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1646 - acc: 0.9989 - val_loss: 0.5220 - val_acc: 0.9167\n",
      "Epoch 135/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1637 - acc: 0.9991 - val_loss: 0.5237 - val_acc: 0.9170\n",
      "Epoch 136/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1637 - acc: 0.9987 - val_loss: 0.5266 - val_acc: 0.9149\n",
      "Epoch 137/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1626 - acc: 0.9990 - val_loss: 0.5181 - val_acc: 0.9190\n",
      "Epoch 138/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1621 - acc: 0.9990 - val_loss: 0.5215 - val_acc: 0.9170\n",
      "Epoch 139/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1614 - acc: 0.9992 - val_loss: 0.5248 - val_acc: 0.9160\n",
      "Epoch 140/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1611 - acc: 0.9991 - val_loss: 0.5201 - val_acc: 0.9167\n",
      "Epoch 141/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1600 - acc: 0.9992 - val_loss: 0.5244 - val_acc: 0.9164\n",
      "Epoch 142/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1595 - acc: 0.9992 - val_loss: 0.5245 - val_acc: 0.9163\n",
      "Epoch 143/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1594 - acc: 0.9991 - val_loss: 0.5195 - val_acc: 0.9185\n",
      "Epoch 144/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1589 - acc: 0.9991 - val_loss: 0.5251 - val_acc: 0.9159\n",
      "Epoch 145/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1581 - acc: 0.9993 - val_loss: 0.5237 - val_acc: 0.9175\n",
      "Epoch 146/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1575 - acc: 0.9992 - val_loss: 0.5261 - val_acc: 0.9157\n",
      "Epoch 147/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1568 - acc: 0.9992 - val_loss: 0.5212 - val_acc: 0.9168\n",
      "Epoch 148/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1560 - acc: 0.9995 - val_loss: 0.5296 - val_acc: 0.9165\n",
      "Epoch 149/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1557 - acc: 0.9994 - val_loss: 0.5246 - val_acc: 0.9162- loss: 0.1557 - acc: 0.9\n",
      "Epoch 150/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1553 - acc: 0.9993 - val_loss: 0.5221 - val_acc: 0.9166\n",
      "Epoch 151/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1553 - acc: 0.9993 - val_loss: 0.5244 - val_acc: 0.9161\n",
      "Epoch 152/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1541 - acc: 0.9995 - val_loss: 0.5164 - val_acc: 0.9174\n",
      "Epoch 153/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1537 - acc: 0.9994 - val_loss: 0.5221 - val_acc: 0.9175\n",
      "Epoch 154/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1529 - acc: 0.9995 - val_loss: 0.5298 - val_acc: 0.9148\n",
      "Epoch 155/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1526 - acc: 0.9993 - val_loss: 0.5293 - val_acc: 0.9155\n",
      "Epoch 156/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1526 - acc: 0.9992 - val_loss: 0.5207 - val_acc: 0.9160\n",
      "Epoch 157/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1518 - acc: 0.9994 - val_loss: 0.5165 - val_acc: 0.9170\n",
      "Epoch 158/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1509 - acc: 0.9994 - val_loss: 0.5201 - val_acc: 0.9172\n",
      "Epoch 159/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1510 - acc: 0.9992 - val_loss: 0.5255 - val_acc: 0.9166\n",
      "Epoch 160/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1500 - acc: 0.9994 - val_loss: 0.5272 - val_acc: 0.9173\n",
      "Epoch 161/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1496 - acc: 0.9994 - val_loss: 0.5195 - val_acc: 0.9183\n",
      "Epoch 162/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1491 - acc: 0.9995 - val_loss: 0.5254 - val_acc: 0.9174\n",
      "Epoch 163/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1486 - acc: 0.9994 - val_loss: 0.5289 - val_acc: 0.9167\n",
      "Epoch 164/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1481 - acc: 0.9994 - val_loss: 0.5253 - val_acc: 0.9167\n",
      "Epoch 165/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1478 - acc: 0.9994 - val_loss: 0.5212 - val_acc: 0.9163\n",
      "Epoch 166/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1471 - acc: 0.9995 - val_loss: 0.5241 - val_acc: 0.9166\n",
      "Epoch 167/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1464 - acc: 0.9996 - val_loss: 0.5220 - val_acc: 0.9167\n",
      "Epoch 168/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1461 - acc: 0.9994 - val_loss: 0.5219 - val_acc: 0.917562 - acc: 0.999 -  - ETA: 1s \n",
      "Epoch 169/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1455 - acc: 0.9996 - val_loss: 0.5283 - val_acc: 0.9160\n",
      "Epoch 170/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1452 - acc: 0.9994 - val_loss: 0.5202 - val_acc: 0.9175\n",
      "Epoch 171/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1449 - acc: 0.9993 - val_loss: 0.5265 - val_acc: 0.9160\n",
      "Epoch 172/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1443 - acc: 0.9995 - val_loss: 0.5187 - val_acc: 0.9181\n",
      "Epoch 173/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1440 - acc: 0.9995 - val_loss: 0.5224 - val_acc: 0.9167\n",
      "Epoch 174/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1432 - acc: 0.9996 - val_loss: 0.5230 - val_acc: 0.9184\n",
      "Epoch 175/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1427 - acc: 0.9995 - val_loss: 0.5199 - val_acc: 0.9191\n",
      "Epoch 176/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1425 - acc: 0.9994 - val_loss: 0.5173 - val_acc: 0.9171\n",
      "Epoch 177/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1422 - acc: 0.9994 - val_loss: 0.5178 - val_acc: 0.9180\n",
      "Epoch 178/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1414 - acc: 0.9995 - val_loss: 0.5218 - val_acc: 0.9187\n",
      "Epoch 179/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1408 - acc: 0.9995 - val_loss: 0.5244 - val_acc: 0.9161\n",
      "Epoch 180/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1406 - acc: 0.9995 - val_loss: 0.5223 - val_acc: 0.9171\n",
      "Epoch 181/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1404 - acc: 0.9994 - val_loss: 0.5185 - val_acc: 0.9172\n",
      "Epoch 182/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1398 - acc: 0.9994 - val_loss: 0.5200 - val_acc: 0.9176\n",
      "Epoch 183/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1395 - acc: 0.9995 - val_loss: 0.5228 - val_acc: 0.9169: 0s - loss: 0.1395 - acc: 0.99\n",
      "Epoch 184/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1389 - acc: 0.9993 - val_loss: 0.5130 - val_acc: 0.9189\n",
      "Epoch 185/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1385 - acc: 0.9996 - val_loss: 0.5176 - val_acc: 0.9178\n",
      "Epoch 186/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1376 - acc: 0.9997 - val_loss: 0.5226 - val_acc: 0.9179\n",
      "Epoch 187/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1374 - acc: 0.9995 - val_loss: 0.5168 - val_acc: 0.9185\n",
      "Epoch 188/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1367 - acc: 0.9997 - val_loss: 0.5216 - val_acc: 0.9174\n",
      "Epoch 189/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1367 - acc: 0.9995 - val_loss: 0.5187 - val_acc: 0.9174\n",
      "Epoch 190/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1360 - acc: 0.9997 - val_loss: 0.5142 - val_acc: 0.9192\n",
      "Epoch 191/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1354 - acc: 0.9997 - val_loss: 0.5141 - val_acc: 0.9183\n",
      "Epoch 192/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 0.1350 - acc: 0.9997 - val_loss: 0.5159 - val_acc: 0.9179\n",
      "Epoch 193/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 0.1345 - acc: 0.9998 - val_loss: 0.5227 - val_acc: 0.9181\n",
      "Epoch 194/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1342 - acc: 0.9995 - val_loss: 0.5179 - val_acc: 0.9191\n",
      "Epoch 195/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1339 - acc: 0.9997 - val_loss: 0.5245 - val_acc: 0.9169\n",
      "Epoch 196/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1334 - acc: 0.9996 - val_loss: 0.5238 - val_acc: 0.9173\n",
      "Epoch 197/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1331 - acc: 0.9996 - val_loss: 0.5122 - val_acc: 0.9187\n",
      "Epoch 198/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1324 - acc: 0.9997 - val_loss: 0.5146 - val_acc: 0.9183\n",
      "Epoch 199/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1321 - acc: 0.9997 - val_loss: 0.5159 - val_acc: 0.9179\n",
      "Epoch 200/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1314 - acc: 0.9998 - val_loss: 0.5190 - val_acc: 0.9165\n",
      "Epoch 201/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1313 - acc: 0.9997 - val_loss: 0.5180 - val_acc: 0.9191\n",
      "Epoch 202/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1314 - acc: 0.9997 - val_loss: 0.5228 - val_acc: 0.9171\n",
      "Epoch 203/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1315 - acc: 0.9996 - val_loss: 0.5178 - val_acc: 0.9179\n",
      "Epoch 204/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1314 - acc: 0.9996 - val_loss: 0.5144 - val_acc: 0.9173\n",
      "Epoch 205/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1316 - acc: 0.9996 - val_loss: 0.5136 - val_acc: 0.9183\n",
      "Epoch 206/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1312 - acc: 0.9996 - val_loss: 0.5190 - val_acc: 0.9174\n",
      "Epoch 207/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1312 - acc: 0.9997 - val_loss: 0.5167 - val_acc: 0.9184\n",
      "Epoch 208/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1308 - acc: 0.9997 - val_loss: 0.5194 - val_acc: 0.9183\n",
      "Epoch 209/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1309 - acc: 0.9997 - val_loss: 0.5152 - val_acc: 0.9179\n",
      "Epoch 210/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1309 - acc: 0.9998 - val_loss: 0.5128 - val_acc: 0.9196\n",
      "Epoch 211/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1309 - acc: 0.9996 - val_loss: 0.5191 - val_acc: 0.9179\n",
      "Epoch 212/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1309 - acc: 0.9996 - val_loss: 0.5137 - val_acc: 0.9171\n",
      "Epoch 213/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1307 - acc: 0.9997 - val_loss: 0.5132 - val_acc: 0.9190\n",
      "Epoch 214/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1308 - acc: 0.9996 - val_loss: 0.5177 - val_acc: 0.9173\n",
      "Epoch 215/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1306 - acc: 0.9997 - val_loss: 0.5184 - val_acc: 0.9181\n",
      "Epoch 216/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1307 - acc: 0.9997 - val_loss: 0.5209 - val_acc: 0.9182\n",
      "Epoch 217/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1305 - acc: 0.9998 - val_loss: 0.5127 - val_acc: 0.9183\n",
      "Epoch 218/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1303 - acc: 0.9998 - val_loss: 0.5163 - val_acc: 0.9186\n",
      "Epoch 219/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1306 - acc: 0.9997 - val_loss: 0.5193 - val_acc: 0.9179\n",
      "Epoch 220/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1307 - acc: 0.9995 - val_loss: 0.5096 - val_acc: 0.9194\n",
      "Epoch 221/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1307 - acc: 0.9995 - val_loss: 0.5187 - val_acc: 0.9177\n",
      "Epoch 222/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1304 - acc: 0.9995 - val_loss: 0.5174 - val_acc: 0.9177\n",
      "Epoch 223/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1303 - acc: 0.9997 - val_loss: 0.5171 - val_acc: 0.9182oss: 0.1 - ETA: 2s - loss: 0.13 - ETA: 1s - lo\n",
      "Epoch 224/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1304 - acc: 0.9997 - val_loss: 0.5141 - val_acc: 0.9181\n",
      "Epoch 225/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1301 - acc: 0.9997 - val_loss: 0.5156 - val_acc: 0.9181\n",
      "Epoch 226/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1300 - acc: 0.9997 - val_loss: 0.5111 - val_acc: 0.9184\n",
      "Epoch 227/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1299 - acc: 0.9998 - val_loss: 0.5200 - val_acc: 0.9174\n",
      "Epoch 228/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1300 - acc: 0.9998 - val_loss: 0.5125 - val_acc: 0.9181\n",
      "Epoch 229/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1304 - acc: 0.9996 - val_loss: 0.5208 - val_acc: 0.9179\n",
      "Epoch 230/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1299 - acc: 0.9999 - val_loss: 0.5177 - val_acc: 0.9189\n",
      "Epoch 231/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1301 - acc: 0.9997 - val_loss: 0.5189 - val_acc: 0.9177\n",
      "Epoch 232/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1298 - acc: 0.9998 - val_loss: 0.5158 - val_acc: 0.9180\n",
      "Epoch 233/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.1297 - acc: 0.9998 - val_loss: 0.5123 - val_acc: 0.9181\n",
      "Epoch 234/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1299 - acc: 0.9997 - val_loss: 0.5094 - val_acc: 0.9178\n",
      "Epoch 235/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1296 - acc: 0.9998 - val_loss: 0.5160 - val_acc: 0.9185\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1294 - acc: 0.9998 - val_loss: 0.5146 - val_acc: 0.9176\n",
      "Epoch 237/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1298 - acc: 0.9997 - val_loss: 0.5130 - val_acc: 0.9170\n",
      "Epoch 238/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1299 - acc: 0.9997 - val_loss: 0.5165 - val_acc: 0.9182\n",
      "Epoch 239/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1296 - acc: 0.9998 - val_loss: 0.5173 - val_acc: 0.9171ETA: 8s  - ETA: 1s - loss:\n",
      "Epoch 240/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1296 - acc: 0.9996 - val_loss: 0.5104 - val_acc: 0.9192\n",
      "Epoch 241/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1293 - acc: 0.9999 - val_loss: 0.5174 - val_acc: 0.9183\n",
      "Epoch 242/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1295 - acc: 0.9997 - val_loss: 0.5168 - val_acc: 0.9178\n",
      "Epoch 243/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1295 - acc: 0.9997 - val_loss: 0.5178 - val_acc: 0.9188- loss: \n",
      "Epoch 244/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1296 - acc: 0.9996 - val_loss: 0.5143 - val_acc: 0.9182\n",
      "Epoch 245/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1295 - acc: 0.9996 - val_loss: 0.5181 - val_acc: 0.9183ss: 0.1295 - acc: 0.99 \n",
      "Epoch 246/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1293 - acc: 0.9997 - val_loss: 0.5171 - val_acc: 0.9176 3s - loss: 0.1293 - acc: 0 - ETA: 3s - los - ETA: 1s - l\n",
      "Epoch 247/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1294 - acc: 0.9997 - val_loss: 0.5186 - val_acc: 0.9170\n",
      "Epoch 248/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1294 - acc: 0.9995 - val_loss: 0.5182 - val_acc: 0.9187\n",
      "Epoch 249/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1293 - acc: 0.9995 - val_loss: 0.5121 - val_acc: 0.9199\n",
      "Epoch 250/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1293 - acc: 0.9997 - val_loss: 0.5161 - val_acc: 0.9183\n",
      "Epoch 251/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1292 - acc: 0.9997 - val_loss: 0.5152 - val_acc: 0.9176\n",
      "Epoch 252/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1289 - acc: 0.9998 - val_loss: 0.5117 - val_acc: 0.9187\n",
      "Epoch 253/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1290 - acc: 0.9997 - val_loss: 0.5202 - val_acc: 0.9180s: 0.1290 - acc: 0 - ETA: 4s - loss: 0.12 - ETA: 3s - loss: 0.1290 - acc: 0.999 - ETA: 3s - loss: 0.1290 - acc: 0 - ETA: 3s - loss: 0.1290 -\n",
      "Epoch 254/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1288 - acc: 0.9998 - val_loss: 0.5115 - val_acc: 0.9186\n",
      "Epoch 255/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1288 - acc: 0.9998 - val_loss: 0.5172 - val_acc: 0.9182\n",
      "Epoch 256/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1290 - acc: 0.9996 - val_loss: 0.5170 - val_acc: 0.9175\n",
      "Epoch 257/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1289 - acc: 0.9997 - val_loss: 0.5123 - val_acc: 0.9181\n",
      "Epoch 258/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1290 - acc: 0.9997 - val_loss: 0.5165 - val_acc: 0.9181\n",
      "Epoch 259/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1291 - acc: 0.9995 - val_loss: 0.5153 - val_acc: 0.9187\n",
      "Epoch 260/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1287 - acc: 0.9998 - val_loss: 0.5146 - val_acc: 0.9192\n",
      "Epoch 261/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1289 - acc: 0.9996 - val_loss: 0.5227 - val_acc: 0.9168\n",
      "Epoch 262/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1288 - acc: 0.9996 - val_loss: 0.5110 - val_acc: 0.9189\n",
      "Epoch 263/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1285 - acc: 0.9997 - val_loss: 0.5168 - val_acc: 0.9174\n",
      "Epoch 264/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1287 - acc: 0.9997 - val_loss: 0.5170 - val_acc: 0.9175\n",
      "Epoch 265/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1285 - acc: 0.9997 - val_loss: 0.5141 - val_acc: 0.9191\n",
      "Epoch 266/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1284 - acc: 0.9997 - val_loss: 0.5122 - val_acc: 0.9187 3s - los - E\n",
      "Epoch 267/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1286 - acc: 0.9998 - val_loss: 0.5138 - val_acc: 0.9180\n",
      "Epoch 268/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1283 - acc: 0.9997 - val_loss: 0.5148 - val_acc: 0.9184\n",
      "Epoch 269/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1283 - acc: 0.9997 - val_loss: 0.5152 - val_acc: 0.9190\n",
      "Epoch 270/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1284 - acc: 0.9997 - val_loss: 0.5128 - val_acc: 0.9176: 10s - loss: 0.1284 - acc:  - ET\n",
      "Epoch 271/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1283 - acc: 0.9996 - val_loss: 0.5128 - val_acc: 0.91750.1284 - acc: 0.999 - ETA: 3s - loss: 0\n",
      "Epoch 272/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1283 - acc: 0.9997 - val_loss: 0.5107 - val_acc: 0.9191\n",
      "Epoch 273/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1283 - acc: 0.9996 - val_loss: 0.5090 - val_acc: 0.9198\n",
      "Epoch 274/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1280 - acc: 0.9998 - val_loss: 0.5114 - val_acc: 0.9187\n",
      "Epoch 275/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1280 - acc: 0.9998 - val_loss: 0.5127 - val_acc: 0.9182\n",
      "Epoch 276/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1281 - acc: 0.9997 - val_loss: 0.5200 - val_acc: 0.9191\n",
      "Epoch 277/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1279 - acc: 0.9997 - val_loss: 0.5155 - val_acc: 0.9178\n",
      "Epoch 278/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1282 - acc: 0.9997 - val_loss: 0.5096 - val_acc: 0.9198\n",
      "Epoch 279/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.1281 - acc: 0.9998 - val_loss: 0.5116 - val_acc: 0.9192\n",
      "Epoch 280/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1279 - acc: 0.9997 - val_loss: 0.5162 - val_acc: 0.9183\n",
      "Epoch 281/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1277 - acc: 0.9998 - val_loss: 0.5225 - val_acc: 0.9183\n",
      "Epoch 282/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1279 - acc: 0.9997 - val_loss: 0.5117 - val_acc: 0.9186\n",
      "Epoch 283/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1278 - acc: 0.9999 - val_loss: 0.5159 - val_acc: 0.9181\n",
      "Epoch 284/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1278 - acc: 0.9997 - val_loss: 0.5128 - val_acc: 0.9188\n",
      "Epoch 285/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1275 - acc: 0.9998 - val_loss: 0.5140 - val_acc: 0.9189: 5s - loss: 0.1275 - a - ETA: 5s - loss: 0.1275 - acc - ETA: \n",
      "Epoch 286/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1277 - acc: 0.9997 - val_loss: 0.5141 - val_acc: 0.9200\n",
      "Epoch 287/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1276 - acc: 0.9997 - val_loss: 0.5179 - val_acc: 0.9173 loss: 0.\n",
      "Epoch 288/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1277 - acc: 0.9996 - val_loss: 0.5112 - val_acc: 0.9187 1s - loss: 0.1277 - acc: 0 - ETA: 0s - loss: 0.1277\n",
      "Epoch 289/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1275 - acc: 0.9997 - val_loss: 0.5113 - val_acc: 0.9187\n",
      "Epoch 290/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1274 - acc: 0.9997 - val_loss: 0.5168 - val_acc: 0.9187\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1275 - acc: 0.9997 - val_loss: 0.5155 - val_acc: 0.9190\n",
      "Epoch 292/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1276 - acc: 0.9996 - val_loss: 0.5125 - val_acc: 0.9192\n",
      "Epoch 293/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1275 - acc: 0.9997 - val_loss: 0.5127 - val_acc: 0.9187\n",
      "Epoch 294/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1274 - acc: 0.9996 - val_loss: 0.5183 - val_acc: 0.9183\n",
      "Epoch 295/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1274 - acc: 0.9998 - val_loss: 0.5152 - val_acc: 0.9183\n",
      "Epoch 296/300\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.1271 - acc: 0.9998 - val_loss: 0.5087 - val_acc: 0.9186acc: 0.99\n",
      "Epoch 297/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1271 - acc: 0.9998 - val_loss: 0.5143 - val_acc: 0.9184: 1s - \n",
      "Epoch 298/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1272 - acc: 0.9997 - val_loss: 0.5121 - val_acc: 0.9196\n",
      "Epoch 299/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1274 - acc: 0.9996 - val_loss: 0.5129 - val_acc: 0.9198\n",
      "Epoch 300/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.1270 - acc: 0.9998 - val_loss: 0.5129 - val_acc: 0.9192\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# set callback\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# start training\n",
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('mobilenet_slim_n_thinner.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
