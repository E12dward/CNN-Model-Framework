{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import os  \n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, Dense, BatchNormalization, Activation\n",
    "from keras.layers import GlobalAveragePooling2D, MaxPooling2D, add\n",
    "from keras.models import Model\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "from keras import optimizers,regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "\n",
    "num_classes        = 10\n",
    "batch_size         = 64         # 64 or 32 or other\n",
    "epochs             = 300\n",
    "iterations         = 782       \n",
    "weight_decay=1e-4\n",
    "\n",
    "log_filepath  = './xception_he_wd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.01\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    return 0.0001\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 36 convolutional layers are structured into 14 modules\n",
    "def entryflow(x,params,top=False):\n",
    "    # modules 2-4,13\n",
    "    # params is (3,)\n",
    "    # top = true means module 2, don't use relu\n",
    "    residual = Conv2D(params[0], (1, 1), strides=(2, 2),padding='same',\n",
    "                     kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    residual = BatchNormalization(momentum=0.9, epsilon=1e-5)(residual)\n",
    "    if top:\n",
    "        x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params[1], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params[2], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2),padding='same')(x)\n",
    "    x = add([x, residual])\n",
    "    return x\n",
    "    \n",
    "def middleflow(x,params):\n",
    "    # modules 5-12, params is int\n",
    "    residual = x\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params, (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params, (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params, (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = add([x, residual])\n",
    "    return x\n",
    "\n",
    "def exitflow(x,params):\n",
    "    # modules 14 , params is (2,)\n",
    "    x = SeparableConv2D(params[0], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params[1], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xception(img_input,shallow=False, classes=10):\n",
    "    # modules 1\n",
    "    x = Conv2D(32,(3, 3),strides=(2, 2),padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3),strides=(1,1),padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # module 2\n",
    "    x = entryflow(x,(128,128,128),top=True)\n",
    "    # module 3-4\n",
    "    x = entryflow(x,(256,256,256))\n",
    "    x = entryflow(x,(728,728,728))\n",
    "    # module 5-12\n",
    "    for _ in range(8):\n",
    "        x = middleflow(x,728)\n",
    "    # module 13\n",
    "    x = entryflow(x,(1024,728,1024))\n",
    "    # module 14\n",
    "    x = exitflow(x,(1536,2048))\n",
    "    # output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(classes, activation='softmax')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   18496       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 16, 16, 128)  8896        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 128)  17664       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 128)    8320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 128)    512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 128)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 256)    34176       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 256)    1024        separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 256)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 8, 8, 256)    68096       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 256)    33024       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 256)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4, 4, 256)    1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 4, 256)    0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 4, 4, 728)    189400      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 4, 728)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 4, 4, 728)    537264      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 2, 2, 728)    187096      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 728)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 2, 2, 728)    2912        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 2, 2, 728)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 2, 2, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 2, 2, 728)    537264      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 2, 2, 728)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 2, 2, 728)    537264      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 2, 2, 728)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 2, 2, 728)    537264      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 2, 2, 728)    0           batch_normalization_14[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 2, 2, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 2, 2, 728)    537264      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 2, 2, 728)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 2, 2, 728)    537264      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 2, 2, 728)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 2, 2, 728)    537264      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 2, 2, 728)    0           batch_normalization_17[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 2, 2, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 2, 2, 728)    537264      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2, 2, 728)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 2, 2, 728)    537264      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 2, 2, 728)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 2, 2, 728)    537264      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 2, 2, 728)    0           batch_normalization_20[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2, 2, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 2, 2, 728)    537264      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2, 2, 728)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 2, 2, 728)    537264      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 2, 2, 728)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 2, 2, 728)    537264      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2, 2, 728)    0           batch_normalization_23[0][0]     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 2, 2, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 2, 2, 728)    537264      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 2, 2, 728)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 2, 2, 728)    537264      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 2, 2, 728)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 2, 2, 728)    537264      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 2, 728)    0           batch_normalization_26[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 2, 2, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 2, 2, 728)    537264      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 2, 2, 728)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 2, 2, 728)    537264      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 2, 2, 728)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 2, 2, 728)    537264      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 2, 2, 728)    0           batch_normalization_29[0][0]     \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 2, 2, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 2, 2, 728)    537264      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 2, 2, 728)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 2, 2, 728)    537264      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2, 2, 728)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 2, 2, 728)    537264      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 2, 2, 728)    0           batch_normalization_32[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 2, 2, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 2, 2, 728)    537264      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 2, 2, 728)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 2, 2, 728)    537264      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 2, 2, 728)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 2, 2, 728)    537264      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 2, 2, 728)    0           batch_normalization_35[0][0]     \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 2, 2, 728)    537264      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2, 2, 728)    2912        separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 2, 2, 728)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 2, 2, 1024)   753048      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 2, 1024)   4096        separable_conv2d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 1, 1024)   746496      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 1024)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 1, 1, 1024)   4096        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 1, 1, 1024)   0           max_pooling2d_4[0][0]            \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_33 (SeparableC (None, 1, 1, 1536)   1583616     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 1, 1, 1536)   6144        separable_conv2d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 1, 1, 1536)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_34 (SeparableC (None, 1, 1, 2048)   3161600     activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 1, 1, 2048)   8192        separable_conv2d_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 1, 1, 2048)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           20490       global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 20,909,234\n",
      "Trainable params: 20,854,706\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input=Input(shape=(32,32,3))\n",
    "output = xception(img_input)\n",
    "model=Model(img_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 6.1931 - acc: 0.3398 - val_loss: 5.7889 - val_acc: 0.4308\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 6.1726 - acc: 0.3258 - val_loss: 6.1288 - val_acc: 0.3085\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 54s 70ms/step - loss: 6.1334 - acc: 0.3230 - val_loss: 5.6729 - val_acc: 0.3734\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 54s 70ms/step - loss: 5.8676 - acc: 0.3694 - val_loss: 5.6140 - val_acc: 0.4164\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 5.6345 - acc: 0.4071 - val_loss: 5.4880 - val_acc: 0.4416\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 5.5878 - acc: 0.3887 - val_loss: 5.3183 - val_acc: 0.4333\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 5.5146 - acc: 0.3702 - val_loss: 5.4309 - val_acc: 0.3855\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 5.3991 - acc: 0.3939 - val_loss: 5.3432 - val_acc: 0.4044\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 5.1331 - acc: 0.4295 - val_loss: 5.0064 - val_acc: 0.4642\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 5.1293 - acc: 0.4192 - val_loss: 5.0693 - val_acc: 0.4519\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 5.0461 - acc: 0.4217 - val_loss: 4.9644 - val_acc: 0.4552\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 5.0668 - acc: 0.3921 - val_loss: 5.0282 - val_acc: 0.3684\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 54s 70ms/step - loss: 4.9971 - acc: 0.3904 - val_loss: 4.9399 - val_acc: 0.4462\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 4.8046 - acc: 0.4343 - val_loss: 4.6108 - val_acc: 0.4873\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 4.6728 - acc: 0.4509 - val_loss: 4.4834 - val_acc: 0.4830\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 4.5552 - acc: 0.4620 - val_loss: 4.3276 - val_acc: 0.5100\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 4.4884 - acc: 0.4538 - val_loss: 4.2419 - val_acc: 0.4951\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 4.4256 - acc: 0.4613 - val_loss: 4.2931 - val_acc: 0.5021\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 4.3871 - acc: 0.4405 - val_loss: 4.0830 - val_acc: 0.5079\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 4.4513 - acc: 0.4049 - val_loss: 4.2861 - val_acc: 0.4249\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 4.3015 - acc: 0.4343 - val_loss: 4.0373 - val_acc: 0.4917\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 4.1262 - acc: 0.4777 - val_loss: 4.2538 - val_acc: 0.4189\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 4.0745 - acc: 0.4716 - val_loss: 3.9232 - val_acc: 0.5134\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 3.9961 - acc: 0.4916 - val_loss: 3.9248 - val_acc: 0.5201\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 3.9030 - acc: 0.4888 - val_loss: 4.2611 - val_acc: 0.3537\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 4.0623 - acc: 0.4336 - val_loss: 3.8843 - val_acc: 0.4720\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 3.9069 - acc: 0.4644 - val_loss: 3.6737 - val_acc: 0.5128\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 3.8441 - acc: 0.4660 - val_loss: 3.6060 - val_acc: 0.5171\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 3.7936 - acc: 0.4696 - val_loss: 3.5867 - val_acc: 0.5013\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 3.6532 - acc: 0.5102 - val_loss: 3.4890 - val_acc: 0.5425\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.5727 - acc: 0.5179 - val_loss: 3.4128 - val_acc: 0.5470\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 3.4439 - acc: 0.5402 - val_loss: 3.3542 - val_acc: 0.5471\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 3.3954 - acc: 0.5411 - val_loss: 3.3208 - val_acc: 0.5539\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.3105 - acc: 0.5520 - val_loss: 3.0941 - val_acc: 0.5820\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.2320 - acc: 0.5644 - val_loss: 3.0892 - val_acc: 0.5856\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.2030 - acc: 0.5573 - val_loss: 3.0728 - val_acc: 0.5820\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 3.0476 - acc: 0.5889 - val_loss: 2.9378 - val_acc: 0.6045\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 3.5832 - acc: 0.4055 - val_loss: 3.6182 - val_acc: 0.4070\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 3.6101 - acc: 0.3984 - val_loss: 3.3594 - val_acc: 0.4617\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.4388 - acc: 0.4435 - val_loss: 3.4779 - val_acc: 0.4764\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 54s 70ms/step - loss: 3.6364 - acc: 0.3943 - val_loss: 3.7974 - val_acc: 0.4478\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 3.4432 - acc: 0.4554 - val_loss: 3.4860 - val_acc: 0.4883\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 3.2804 - acc: 0.4940 - val_loss: 3.2214 - val_acc: 0.5015\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 3.6008 - acc: 0.3902 - val_loss: 3.6230 - val_acc: 0.4039\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 3.5283 - acc: 0.4040 - val_loss: 3.5107 - val_acc: 0.4131\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 3.4798 - acc: 0.4195 - val_loss: 3.3812 - val_acc: 0.4374\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 3.4771 - acc: 0.4219 - val_loss: 3.5880 - val_acc: 0.4179\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 3.5365 - acc: 0.4086 - val_loss: 3.1943 - val_acc: 0.4730\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.3971 - acc: 0.4456 - val_loss: 3.2144 - val_acc: 0.4697\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 3.4598 - acc: 0.4175 - val_loss: 3.4418 - val_acc: 0.4333\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 3.4224 - acc: 0.4299 - val_loss: 3.1658 - val_acc: 0.4746\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 3.3088 - acc: 0.4510 - val_loss: 3.3516 - val_acc: 0.4651\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 3.1644 - acc: 0.4707 - val_loss: 3.4993 - val_acc: 0.4786\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 3.2129 - acc: 0.4643 - val_loss: 3.1720 - val_acc: 0.4648\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 3.1431 - acc: 0.4679 - val_loss: 3.1120 - val_acc: 0.4883\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 3.1059 - acc: 0.4827 - val_loss: 3.4003 - val_acc: 0.4285\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 3.1637 - acc: 0.4512 - val_loss: 2.9713 - val_acc: 0.4895\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 3.0631 - acc: 0.4765 - val_loss: 2.8805 - val_acc: 0.4873\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 2.9950 - acc: 0.4871 - val_loss: 3.1675 - val_acc: 0.4927\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 2.8296 - acc: 0.5077 - val_loss: 2.8548 - val_acc: 0.5152\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 2.7561 - acc: 0.5240 - val_loss: 2.9181 - val_acc: 0.5048\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 2.8329 - acc: 0.5050 - val_loss: 2.6356 - val_acc: 0.5172\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 2.5586 - acc: 0.5310 - val_loss: 2.5574 - val_acc: 0.5388\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 2.4812 - acc: 0.5429 - val_loss: 2.5034 - val_acc: 0.5524\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 2.4162 - acc: 0.5559 - val_loss: 2.4322 - val_acc: 0.5605\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 2.3606 - acc: 0.5614 - val_loss: 2.4343 - val_acc: 0.5604\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 2.3021 - acc: 0.5731 - val_loss: 2.3843 - val_acc: 0.5667\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 2.2529 - acc: 0.5793 - val_loss: 2.3221 - val_acc: 0.5638\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 2.2100 - acc: 0.5829 - val_loss: 2.2673 - val_acc: 0.5717\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 2.1581 - acc: 0.5903 - val_loss: 2.2438 - val_acc: 0.5813\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 2.1091 - acc: 0.5991 - val_loss: 2.3149 - val_acc: 0.5772\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 2.0707 - acc: 0.6060 - val_loss: 2.1930 - val_acc: 0.5839\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 2.0277 - acc: 0.6138 - val_loss: 2.1980 - val_acc: 0.5800\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 1.9863 - acc: 0.6174 - val_loss: 2.1615 - val_acc: 0.5929\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.9473 - acc: 0.6231 - val_loss: 2.1199 - val_acc: 0.5934\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 1.9199 - acc: 0.6238 - val_loss: 2.1323 - val_acc: 0.5892\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.8797 - acc: 0.6329 - val_loss: 2.0613 - val_acc: 0.5973\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.8411 - acc: 0.6401 - val_loss: 2.0365 - val_acc: 0.6058\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 1.8028 - acc: 0.6494 - val_loss: 1.9498 - val_acc: 0.6201\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.7881 - acc: 0.6465 - val_loss: 1.9175 - val_acc: 0.6121\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 1.7514 - acc: 0.6544 - val_loss: 2.0572 - val_acc: 0.6046\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 1.7151 - acc: 0.6618 - val_loss: 1.9729 - val_acc: 0.6070\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 54s 69ms/step - loss: 1.6774 - acc: 0.6690 - val_loss: 1.8849 - val_acc: 0.6140\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.6492 - acc: 0.6720 - val_loss: 2.0275 - val_acc: 0.6076\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 1.6190 - acc: 0.6771 - val_loss: 1.8744 - val_acc: 0.6233\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 1.5893 - acc: 0.6835 - val_loss: 1.8465 - val_acc: 0.6208\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 1.5620 - acc: 0.6885 - val_loss: 1.8657 - val_acc: 0.6175\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.5252 - acc: 0.6973 - val_loss: 1.7567 - val_acc: 0.6352\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 1.5020 - acc: 0.7003 - val_loss: 1.8323 - val_acc: 0.6331\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 1.4801 - acc: 0.7061 - val_loss: 1.9041 - val_acc: 0.6123\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 55s 71ms/step - loss: 1.4485 - acc: 0.7133 - val_loss: 1.8655 - val_acc: 0.6250\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 1.4380 - acc: 0.7101 - val_loss: 1.8156 - val_acc: 0.6297\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 1.3969 - acc: 0.7239 - val_loss: 1.7778 - val_acc: 0.6354\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 1.3681 - acc: 0.7299 - val_loss: 1.8824 - val_acc: 0.6238\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.3590 - acc: 0.7302 - val_loss: 1.8581 - val_acc: 0.6340\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 1.3196 - acc: 0.7415 - val_loss: 1.8360 - val_acc: 0.6267\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 1.3029 - acc: 0.7433 - val_loss: 1.7314 - val_acc: 0.6365\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 1.2734 - acc: 0.7512 - val_loss: 1.7429 - val_acc: 0.6437\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 55s 70ms/step - loss: 1.2534 - acc: 0.7580 - val_loss: 1.7515 - val_acc: 0.6441\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 1.2277 - acc: 0.7643 - val_loss: 1.7798 - val_acc: 0.6474\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 1.0747 - acc: 0.8194 - val_loss: 1.8076 - val_acc: 0.6624\n",
      "Epoch 102/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.9984 - acc: 0.8478 - val_loss: 1.7916 - val_acc: 0.6673\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.9670 - acc: 0.8568 - val_loss: 1.8505 - val_acc: 0.6649\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.9441 - acc: 0.8644 - val_loss: 1.8678 - val_acc: 0.6670\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.9285 - acc: 0.8707 - val_loss: 1.8695 - val_acc: 0.6692\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 50s 65ms/step - loss: 0.9100 - acc: 0.8764 - val_loss: 1.9145 - val_acc: 0.6613\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.9010 - acc: 0.8793 - val_loss: 1.9461 - val_acc: 0.6613\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.8807 - acc: 0.8853 - val_loss: 1.9782 - val_acc: 0.6603\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.8633 - acc: 0.8926 - val_loss: 1.9449 - val_acc: 0.6628\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.8567 - acc: 0.8918 - val_loss: 1.9756 - val_acc: 0.6668\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.8402 - acc: 0.8992 - val_loss: 1.9978 - val_acc: 0.6634\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.8282 - acc: 0.9036 - val_loss: 1.9904 - val_acc: 0.6657\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.8195 - acc: 0.9053 - val_loss: 2.0433 - val_acc: 0.6603\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.8132 - acc: 0.9059 - val_loss: 2.1104 - val_acc: 0.6556\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.8041 - acc: 0.9101 - val_loss: 2.0799 - val_acc: 0.6611\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7966 - acc: 0.9115 - val_loss: 2.1164 - val_acc: 0.6595\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7912 - acc: 0.9131 - val_loss: 2.0929 - val_acc: 0.6613\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7830 - acc: 0.9159 - val_loss: 2.0943 - val_acc: 0.6624\n",
      "Epoch 119/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7730 - acc: 0.9187 - val_loss: 2.1188 - val_acc: 0.6602\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7646 - acc: 0.9206 - val_loss: 2.0730 - val_acc: 0.6609\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7496 - acc: 0.9262 - val_loss: 2.0882 - val_acc: 0.6637\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7489 - acc: 0.9277 - val_loss: 2.0721 - val_acc: 0.6588\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7421 - acc: 0.9281 - val_loss: 2.0805 - val_acc: 0.6651\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7332 - acc: 0.9317 - val_loss: 2.1076 - val_acc: 0.6598\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7229 - acc: 0.9342 - val_loss: 2.0776 - val_acc: 0.6598\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7180 - acc: 0.9356 - val_loss: 2.1082 - val_acc: 0.6594\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7135 - acc: 0.9371 - val_loss: 2.1105 - val_acc: 0.6593\n",
      "Epoch 128/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7059 - acc: 0.9386 - val_loss: 2.1280 - val_acc: 0.6582\n",
      "Epoch 129/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.7004 - acc: 0.9419 - val_loss: 2.1608 - val_acc: 0.6534\n",
      "Epoch 130/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6863 - acc: 0.9465 - val_loss: 2.0809 - val_acc: 0.6662\n",
      "Epoch 131/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6859 - acc: 0.9444 - val_loss: 2.1283 - val_acc: 0.6601\n",
      "Epoch 132/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6797 - acc: 0.9476 - val_loss: 2.1482 - val_acc: 0.6616\n",
      "Epoch 133/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.6755 - acc: 0.9480 - val_loss: 2.0796 - val_acc: 0.6616\n",
      "Epoch 134/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.6724 - acc: 0.9487 - val_loss: 2.1252 - val_acc: 0.6649\n",
      "Epoch 135/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.6650 - acc: 0.9515 - val_loss: 2.0851 - val_acc: 0.6618\n",
      "Epoch 136/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6605 - acc: 0.9516 - val_loss: 2.1256 - val_acc: 0.6627\n",
      "Epoch 137/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.6550 - acc: 0.9532 - val_loss: 2.1469 - val_acc: 0.6600\n",
      "Epoch 138/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6517 - acc: 0.9543 - val_loss: 2.1557 - val_acc: 0.6617\n",
      "Epoch 139/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.6463 - acc: 0.9554 - val_loss: 2.2029 - val_acc: 0.6592\n",
      "Epoch 140/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6465 - acc: 0.9547 - val_loss: 2.1761 - val_acc: 0.6621\n",
      "Epoch 141/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.6372 - acc: 0.9577 - val_loss: 2.1631 - val_acc: 0.6621\n",
      "Epoch 142/300\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.6307 - acc: 0.9609 - val_loss: 2.1498 - val_acc: 0.6636\n",
      "Epoch 143/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.6286 - acc: 0.9604 - val_loss: 2.1708 - val_acc: 0.6619\n",
      "Epoch 144/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.6274 - acc: 0.9611 - val_loss: 2.1730 - val_acc: 0.6612\n",
      "Epoch 145/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.6276 - acc: 0.9598 - val_loss: 2.1007 - val_acc: 0.6627\n",
      "Epoch 146/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.6182 - acc: 0.9619 - val_loss: 2.1143 - val_acc: 0.6630\n",
      "Epoch 147/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.6158 - acc: 0.9621 - val_loss: 2.1758 - val_acc: 0.6614\n",
      "Epoch 148/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.6121 - acc: 0.9637 - val_loss: 2.1370 - val_acc: 0.6673\n",
      "Epoch 149/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.6120 - acc: 0.9636 - val_loss: 2.1557 - val_acc: 0.6610\n",
      "Epoch 150/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.6031 - acc: 0.9662 - val_loss: 2.1735 - val_acc: 0.6592\n",
      "Epoch 151/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.6027 - acc: 0.9659 - val_loss: 2.2216 - val_acc: 0.6618\n",
      "Epoch 152/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5966 - acc: 0.9674 - val_loss: 2.1634 - val_acc: 0.6623\n",
      "Epoch 153/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5949 - acc: 0.9665 - val_loss: 2.2098 - val_acc: 0.6572\n",
      "Epoch 154/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5936 - acc: 0.9673 - val_loss: 2.1998 - val_acc: 0.6598\n",
      "Epoch 155/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5872 - acc: 0.9693 - val_loss: 2.1933 - val_acc: 0.6565\n",
      "Epoch 156/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5881 - acc: 0.9690 - val_loss: 2.2190 - val_acc: 0.6601\n",
      "Epoch 157/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5824 - acc: 0.9699 - val_loss: 2.2019 - val_acc: 0.6635\n",
      "Epoch 158/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5813 - acc: 0.9695 - val_loss: 2.2094 - val_acc: 0.6594\n",
      "Epoch 159/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5772 - acc: 0.9699 - val_loss: 2.2069 - val_acc: 0.6591\n",
      "Epoch 160/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5737 - acc: 0.9707 - val_loss: 2.1710 - val_acc: 0.6576\n",
      "Epoch 161/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5714 - acc: 0.9728 - val_loss: 2.2425 - val_acc: 0.6608\n",
      "Epoch 162/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5656 - acc: 0.9735 - val_loss: 2.2363 - val_acc: 0.6590\n",
      "Epoch 163/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5656 - acc: 0.9724 - val_loss: 2.2451 - val_acc: 0.6591\n",
      "Epoch 164/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5630 - acc: 0.9743 - val_loss: 2.2394 - val_acc: 0.6552\n",
      "Epoch 165/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5636 - acc: 0.9725 - val_loss: 2.1859 - val_acc: 0.6602\n",
      "Epoch 166/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5612 - acc: 0.9729 - val_loss: 2.1664 - val_acc: 0.6594\n",
      "Epoch 167/300\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.5566 - acc: 0.9738 - val_loss: 2.2710 - val_acc: 0.6573\n",
      "Epoch 168/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5527 - acc: 0.9757 - val_loss: 2.1866 - val_acc: 0.6619\n",
      "Epoch 169/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5500 - acc: 0.9755 - val_loss: 2.2306 - val_acc: 0.6616\n",
      "Epoch 170/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5458 - acc: 0.9768 - val_loss: 2.1812 - val_acc: 0.6618\n",
      "Epoch 171/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5488 - acc: 0.9750 - val_loss: 2.1971 - val_acc: 0.6570\n",
      "Epoch 172/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5466 - acc: 0.9746 - val_loss: 2.2072 - val_acc: 0.6597\n",
      "Epoch 173/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5417 - acc: 0.9769 - val_loss: 2.1937 - val_acc: 0.6599\n",
      "Epoch 174/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5377 - acc: 0.9786 - val_loss: 2.2190 - val_acc: 0.6583\n",
      "Epoch 175/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.5341 - acc: 0.9791 - val_loss: 2.1694 - val_acc: 0.6601\n",
      "Epoch 176/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5331 - acc: 0.9784 - val_loss: 2.2496 - val_acc: 0.6618\n",
      "Epoch 177/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5319 - acc: 0.9782 - val_loss: 2.1966 - val_acc: 0.6582\n",
      "Epoch 178/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.5337 - acc: 0.9771 - val_loss: 2.2440 - val_acc: 0.6622\n",
      "Epoch 179/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5315 - acc: 0.9778 - val_loss: 2.3563 - val_acc: 0.6525\n",
      "Epoch 180/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5258 - acc: 0.9786 - val_loss: 2.2681 - val_acc: 0.6582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5253 - acc: 0.9781 - val_loss: 2.2604 - val_acc: 0.6597\n",
      "Epoch 182/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5219 - acc: 0.9800 - val_loss: 2.2494 - val_acc: 0.6597\n",
      "Epoch 183/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5194 - acc: 0.9805 - val_loss: 2.3003 - val_acc: 0.6572\n",
      "Epoch 184/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5177 - acc: 0.9805 - val_loss: 2.2998 - val_acc: 0.6567\n",
      "Epoch 185/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.5125 - acc: 0.9820 - val_loss: 2.2166 - val_acc: 0.6640\n",
      "Epoch 186/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5161 - acc: 0.9804 - val_loss: 2.2365 - val_acc: 0.6622\n",
      "Epoch 187/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5117 - acc: 0.9813 - val_loss: 2.2498 - val_acc: 0.6587\n",
      "Epoch 188/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.5083 - acc: 0.9821 - val_loss: 2.3560 - val_acc: 0.6535\n",
      "Epoch 189/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5085 - acc: 0.9811 - val_loss: 2.2853 - val_acc: 0.6544\n",
      "Epoch 190/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.5022 - acc: 0.9833 - val_loss: 2.3167 - val_acc: 0.6553\n",
      "Epoch 191/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.5034 - acc: 0.9820 - val_loss: 2.3417 - val_acc: 0.6570\n",
      "Epoch 192/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4999 - acc: 0.9832 - val_loss: 2.2962 - val_acc: 0.6582\n",
      "Epoch 193/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4979 - acc: 0.9835 - val_loss: 2.2712 - val_acc: 0.6589\n",
      "Epoch 194/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4953 - acc: 0.9837 - val_loss: 2.3440 - val_acc: 0.6558\n",
      "Epoch 195/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4985 - acc: 0.9817 - val_loss: 2.2555 - val_acc: 0.6561\n",
      "Epoch 196/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4940 - acc: 0.9837 - val_loss: 2.3308 - val_acc: 0.6559\n",
      "Epoch 197/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4913 - acc: 0.9836 - val_loss: 2.2956 - val_acc: 0.6577\n",
      "Epoch 198/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4909 - acc: 0.9839 - val_loss: 2.2874 - val_acc: 0.6558\n",
      "Epoch 199/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4864 - acc: 0.9849 - val_loss: 2.2557 - val_acc: 0.6574\n",
      "Epoch 200/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4887 - acc: 0.9831 - val_loss: 2.2700 - val_acc: 0.6568\n",
      "Epoch 201/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4802 - acc: 0.9868 - val_loss: 2.2740 - val_acc: 0.6603\n",
      "Epoch 202/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4753 - acc: 0.9881 - val_loss: 2.2464 - val_acc: 0.6619\n",
      "Epoch 203/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4724 - acc: 0.9893 - val_loss: 2.2480 - val_acc: 0.6590\n",
      "Epoch 204/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4707 - acc: 0.9897 - val_loss: 2.2668 - val_acc: 0.6596\n",
      "Epoch 205/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4678 - acc: 0.9910 - val_loss: 2.2700 - val_acc: 0.6604\n",
      "Epoch 206/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4694 - acc: 0.9902 - val_loss: 2.2165 - val_acc: 0.6635\n",
      "Epoch 207/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4676 - acc: 0.9910 - val_loss: 2.2145 - val_acc: 0.6606\n",
      "Epoch 208/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4666 - acc: 0.9912 - val_loss: 2.2585 - val_acc: 0.6606\n",
      "Epoch 209/300\n",
      "782/782 [==============================] - 50s 65ms/step - loss: 0.4654 - acc: 0.9916 - val_loss: 2.2652 - val_acc: 0.6606\n",
      "Epoch 210/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4642 - acc: 0.9915 - val_loss: 2.2774 - val_acc: 0.6593\n",
      "Epoch 211/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4638 - acc: 0.9923 - val_loss: 2.2503 - val_acc: 0.6612\n",
      "Epoch 212/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4639 - acc: 0.9919 - val_loss: 2.2557 - val_acc: 0.6613\n",
      "Epoch 213/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4631 - acc: 0.9922 - val_loss: 2.2429 - val_acc: 0.6598\n",
      "Epoch 214/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4636 - acc: 0.9922 - val_loss: 2.2855 - val_acc: 0.6636\n",
      "Epoch 215/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4629 - acc: 0.9918 - val_loss: 2.2935 - val_acc: 0.6614\n",
      "Epoch 216/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4612 - acc: 0.9933 - val_loss: 2.2853 - val_acc: 0.6624\n",
      "Epoch 217/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4604 - acc: 0.9932 - val_loss: 2.2987 - val_acc: 0.6592\n",
      "Epoch 218/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4609 - acc: 0.9927 - val_loss: 2.2437 - val_acc: 0.6628\n",
      "Epoch 219/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4601 - acc: 0.9927 - val_loss: 2.2883 - val_acc: 0.6590\n",
      "Epoch 220/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4602 - acc: 0.9926 - val_loss: 2.2503 - val_acc: 0.6626\n",
      "Epoch 221/300\n",
      "782/782 [==============================] - 50s 65ms/step - loss: 0.4592 - acc: 0.9933 - val_loss: 2.2419 - val_acc: 0.6620\n",
      "Epoch 222/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4592 - acc: 0.9928 - val_loss: 2.2088 - val_acc: 0.6663\n",
      "Epoch 223/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4593 - acc: 0.9930 - val_loss: 2.2649 - val_acc: 0.6618\n",
      "Epoch 224/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4583 - acc: 0.9933 - val_loss: 2.2184 - val_acc: 0.6624\n",
      "Epoch 225/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4572 - acc: 0.9935 - val_loss: 2.2454 - val_acc: 0.6615\n",
      "Epoch 226/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4588 - acc: 0.9928 - val_loss: 2.2461 - val_acc: 0.6620\n",
      "Epoch 227/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4577 - acc: 0.9932 - val_loss: 2.2130 - val_acc: 0.6607\n",
      "Epoch 228/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4575 - acc: 0.9932 - val_loss: 2.3292 - val_acc: 0.6601\n",
      "Epoch 229/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4568 - acc: 0.9936 - val_loss: 2.2452 - val_acc: 0.6652\n",
      "Epoch 230/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4571 - acc: 0.9933 - val_loss: 2.2474 - val_acc: 0.6599\n",
      "Epoch 231/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4573 - acc: 0.9931 - val_loss: 2.2269 - val_acc: 0.6622\n",
      "Epoch 232/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4574 - acc: 0.9932 - val_loss: 2.3021 - val_acc: 0.6605\n",
      "Epoch 233/300\n",
      "782/782 [==============================] - 54s 68ms/step - loss: 0.4558 - acc: 0.9936 - val_loss: 2.3200 - val_acc: 0.6588\n",
      "Epoch 234/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4557 - acc: 0.9935 - val_loss: 2.2396 - val_acc: 0.6620\n",
      "Epoch 235/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4541 - acc: 0.9940 - val_loss: 2.2626 - val_acc: 0.6624\n",
      "Epoch 236/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4569 - acc: 0.9926 - val_loss: 2.2189 - val_acc: 0.6676\n",
      "Epoch 237/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4534 - acc: 0.9943 - val_loss: 2.2228 - val_acc: 0.6633\n",
      "Epoch 238/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4556 - acc: 0.9935 - val_loss: 2.2633 - val_acc: 0.6620\n",
      "Epoch 239/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4543 - acc: 0.9940 - val_loss: 2.2528 - val_acc: 0.6620\n",
      "Epoch 240/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4538 - acc: 0.9942 - val_loss: 2.2543 - val_acc: 0.6619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4534 - acc: 0.9942 - val_loss: 2.2602 - val_acc: 0.6588\n",
      "Epoch 242/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4541 - acc: 0.9938 - val_loss: 2.2359 - val_acc: 0.6636\n",
      "Epoch 243/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4531 - acc: 0.9942 - val_loss: 2.2306 - val_acc: 0.6609\n",
      "Epoch 244/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4533 - acc: 0.9940 - val_loss: 2.2343 - val_acc: 0.6641\n",
      "Epoch 245/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4530 - acc: 0.9942 - val_loss: 2.2359 - val_acc: 0.6623\n",
      "Epoch 246/300\n",
      "782/782 [==============================] - 50s 65ms/step - loss: 0.4523 - acc: 0.9942 - val_loss: 2.2424 - val_acc: 0.6602\n",
      "Epoch 247/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4520 - acc: 0.9943 - val_loss: 2.2075 - val_acc: 0.6630\n",
      "Epoch 248/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4529 - acc: 0.9940 - val_loss: 2.2450 - val_acc: 0.6624\n",
      "Epoch 249/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4514 - acc: 0.9942 - val_loss: 2.2625 - val_acc: 0.6602\n",
      "Epoch 250/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4518 - acc: 0.9940 - val_loss: 2.2742 - val_acc: 0.6630\n",
      "Epoch 251/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4512 - acc: 0.9947 - val_loss: 2.2549 - val_acc: 0.6634\n",
      "Epoch 252/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4519 - acc: 0.9941 - val_loss: 2.2356 - val_acc: 0.6612\n",
      "Epoch 253/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4503 - acc: 0.9943 - val_loss: 2.2425 - val_acc: 0.6628\n",
      "Epoch 254/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4504 - acc: 0.9944 - val_loss: 2.2576 - val_acc: 0.6623\n",
      "Epoch 255/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4502 - acc: 0.9944 - val_loss: 2.2388 - val_acc: 0.6633\n",
      "Epoch 256/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4503 - acc: 0.9949 - val_loss: 2.2727 - val_acc: 0.6622\n",
      "Epoch 257/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4503 - acc: 0.9945 - val_loss: 2.2158 - val_acc: 0.6652\n",
      "Epoch 258/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4485 - acc: 0.9951 - val_loss: 2.2794 - val_acc: 0.6622\n",
      "Epoch 259/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4497 - acc: 0.9944 - val_loss: 2.2306 - val_acc: 0.6633\n",
      "Epoch 260/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4499 - acc: 0.9944 - val_loss: 2.2558 - val_acc: 0.6612\n",
      "Epoch 261/300\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.4485 - acc: 0.9948 - val_loss: 2.2630 - val_acc: 0.6604\n",
      "Epoch 262/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4485 - acc: 0.9946 - val_loss: 2.2254 - val_acc: 0.6631\n",
      "Epoch 263/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4491 - acc: 0.9949 - val_loss: 2.2461 - val_acc: 0.6631\n",
      "Epoch 264/300\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.4489 - acc: 0.9944 - val_loss: 2.2733 - val_acc: 0.6618\n",
      "Epoch 265/300\n",
      "782/782 [==============================] - 50s 65ms/step - loss: 0.4482 - acc: 0.9944 - val_loss: 2.2438 - val_acc: 0.6624\n",
      "Epoch 266/300\n",
      "782/782 [==============================] - 53s 68ms/step - loss: 0.4478 - acc: 0.9953 - val_loss: 2.3615 - val_acc: 0.6567\n",
      "Epoch 267/300\n",
      "782/782 [==============================] - 50s 65ms/step - loss: 0.4480 - acc: 0.9949 - val_loss: 2.2670 - val_acc: 0.6636\n",
      "Epoch 268/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4481 - acc: 0.9946 - val_loss: 2.2750 - val_acc: 0.6626\n",
      "Epoch 269/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4464 - acc: 0.9954 - val_loss: 2.2770 - val_acc: 0.6642\n",
      "Epoch 270/300\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.4478 - acc: 0.9942 - val_loss: 2.2491 - val_acc: 0.6640\n",
      "Epoch 271/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4486 - acc: 0.9941 - val_loss: 2.2983 - val_acc: 0.6638\n",
      "Epoch 272/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4472 - acc: 0.9945 - val_loss: 2.2120 - val_acc: 0.6666\n",
      "Epoch 273/300\n",
      "782/782 [==============================] - 53s 67ms/step - loss: 0.4469 - acc: 0.9947 - val_loss: 2.2530 - val_acc: 0.6640\n",
      "Epoch 274/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4469 - acc: 0.9951 - val_loss: 2.2573 - val_acc: 0.6613\n",
      "Epoch 275/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4461 - acc: 0.9945 - val_loss: 2.2467 - val_acc: 0.6622\n",
      "Epoch 276/300\n",
      "782/782 [==============================] - 50s 64ms/step - loss: 0.4465 - acc: 0.9947 - val_loss: 2.2589 - val_acc: 0.6649\n",
      "Epoch 277/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4464 - acc: 0.9946 - val_loss: 2.2308 - val_acc: 0.6666\n",
      "Epoch 278/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4453 - acc: 0.9952 - val_loss: 2.2455 - val_acc: 0.6642\n",
      "Epoch 279/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4457 - acc: 0.9949 - val_loss: 2.2669 - val_acc: 0.6648\n",
      "Epoch 280/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4456 - acc: 0.9951 - val_loss: 2.2621 - val_acc: 0.6638\n",
      "Epoch 281/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4453 - acc: 0.9945 - val_loss: 2.2714 - val_acc: 0.6603\n",
      "Epoch 282/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4459 - acc: 0.9942 - val_loss: 2.2565 - val_acc: 0.6636\n",
      "Epoch 283/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4445 - acc: 0.9952 - val_loss: 2.2883 - val_acc: 0.6606\n",
      "Epoch 284/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4446 - acc: 0.9951 - val_loss: 2.2198 - val_acc: 0.6646\n",
      "Epoch 285/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4445 - acc: 0.9948 - val_loss: 2.2841 - val_acc: 0.6620\n",
      "Epoch 286/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4438 - acc: 0.9949 - val_loss: 2.3041 - val_acc: 0.6621\n",
      "Epoch 287/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4434 - acc: 0.9958 - val_loss: 2.2549 - val_acc: 0.6620\n",
      "Epoch 288/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4433 - acc: 0.9954 - val_loss: 2.2927 - val_acc: 0.6617\n",
      "Epoch 289/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4433 - acc: 0.9955 - val_loss: 2.2670 - val_acc: 0.6634\n",
      "Epoch 290/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4445 - acc: 0.9949 - val_loss: 2.2534 - val_acc: 0.6641\n",
      "Epoch 291/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4429 - acc: 0.9953 - val_loss: 2.2546 - val_acc: 0.6647\n",
      "Epoch 292/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4437 - acc: 0.9951 - val_loss: 2.2262 - val_acc: 0.6635\n",
      "Epoch 293/300\n",
      "782/782 [==============================] - 52s 67ms/step - loss: 0.4425 - acc: 0.9951 - val_loss: 2.2784 - val_acc: 0.6624\n",
      "Epoch 294/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4429 - acc: 0.9951 - val_loss: 2.2706 - val_acc: 0.6618\n",
      "Epoch 295/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4419 - acc: 0.9951 - val_loss: 2.2709 - val_acc: 0.6641\n",
      "Epoch 296/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4415 - acc: 0.9952 - val_loss: 2.2415 - val_acc: 0.6658\n",
      "Epoch 297/300\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.4414 - acc: 0.9954 - val_loss: 2.2597 - val_acc: 0.6635\n",
      "Epoch 298/300\n",
      "782/782 [==============================] - 51s 66ms/step - loss: 0.4409 - acc: 0.9955 - val_loss: 2.3081 - val_acc: 0.6633\n",
      "Epoch 299/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4419 - acc: 0.9953 - val_loss: 2.2348 - val_acc: 0.6649\n",
      "Epoch 300/300\n",
      "782/782 [==============================] - 51s 65ms/step - loss: 0.4414 - acc: 0.9951 - val_loss: 2.2354 - val_acc: 0.6662\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# set callback\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# start training\n",
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('xception_he_wd.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
