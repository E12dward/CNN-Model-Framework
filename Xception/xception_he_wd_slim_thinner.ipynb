{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import os  \n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, Dense, BatchNormalization, Activation\n",
    "from keras.layers import GlobalAveragePooling2D, MaxPooling2D, add\n",
    "from keras.models import Model\n",
    "from keras.layers import SeparableConv2D\n",
    "\n",
    "from keras import optimizers,regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "\n",
    "num_classes        = 10\n",
    "batch_size         = 64         # 64 or 32 or other\n",
    "epochs             = 300\n",
    "iterations         = 782       \n",
    "weight_decay=1e-4\n",
    "\n",
    "log_filepath  = './xception_he_wd_slim_thinner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std  = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
    "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.01\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    return 0.0001\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 36 convolutional layers are structured into 14 modules\n",
    "def entryflow(x,params,top=False,last=False):\n",
    "    # modules 2-4,13\n",
    "    # params is (3,)\n",
    "    # top = true means module 2, don't use relu\n",
    "    if last:\n",
    "        stride = (2,2)\n",
    "    else:\n",
    "        stride = (1,1)\n",
    "    residual = Conv2D(params[0], (1, 1), strides=stride,padding='same',\n",
    "                     kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    residual = BatchNormalization(momentum=0.9, epsilon=1e-5)(residual)\n",
    "    if top:\n",
    "        x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params[1], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params[2], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=stride,padding='same')(x)\n",
    "    x = add([x, residual])\n",
    "    return x\n",
    "    \n",
    "def middleflow(x,params):\n",
    "    # modules 5-12, params is int\n",
    "    residual = x\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params, (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params, (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params, (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = add([x, residual])\n",
    "    return x\n",
    "\n",
    "def exitflow(x,params):\n",
    "    # modules 14 , params is (2,)\n",
    "    x = SeparableConv2D(params[0], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(params[1], (3, 3),padding='same',\n",
    "                       depthwise_initializer=\"he_normal\",\n",
    "                       pointwise_initializer=\"he_normal\",\n",
    "                       depthwise_regularizer=regularizers.l2(weight_decay),\n",
    "                       pointwise_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xception(img_input,shallow=False, classes=10):\n",
    "    # modules 1\n",
    "    x = Conv2D(32,(3, 3),strides=(2, 2),padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3),strides=(1,1),padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(364, (3, 3),strides=(1,1),padding='same',\n",
    "              kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # module 5-12\n",
    "    for _ in range(4):\n",
    "        x = middleflow(x,364)\n",
    "    # module 13\n",
    "    x = entryflow(x,(512,364,512),last=True)\n",
    "    # module 14\n",
    "    x = exitflow(x,(768,1024))\n",
    "    # output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(classes, activation='softmax')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   18496       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 364)  210028      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 364)  1456        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 364)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 364)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 16, 16, 364)  136136      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 364)  1456        separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 364)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 364)  136136      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 364)  1456        separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 364)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 16, 16, 364)  136136      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 364)  1456        separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 364)  0           batch_normalization_6[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 364)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 16, 16, 364)  136136      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 364)  1456        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 364)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 16, 16, 364)  136136      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 364)  1456        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 364)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 16, 16, 364)  136136      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 364)  1456        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 364)  0           batch_normalization_9[0][0]      \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 364)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 16, 16, 364)  136136      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 364)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 16, 16, 364)  136136      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 364)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 16, 16, 364)  136136      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 364)  0           batch_normalization_12[0][0]     \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 364)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 16, 16, 364)  136136      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 364)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 16, 16, 364)  136136      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 364)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 16, 16, 364)  136136      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 364)  0           batch_normalization_15[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 16, 16, 364)  136136      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 364)  1456        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 364)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 16, 16, 512)  190156      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 512)  2048        separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 512)    186880      add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 512)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 512)    2048        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 512)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 8, 8, 768)    398592      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 768)    3072        separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 768)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 8, 8, 1024)   794368      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 1024)   4096        separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1024)         0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           10250       global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 3,611,466\n",
      "Trainable params: 3,595,450\n",
      "Non-trainable params: 16,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input=Input(shape=(32,32,3))\n",
    "output = xception(img_input)\n",
    "model=Model(img_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "782/782 [==============================] - 71s 91ms/step - loss: 2.6713 - acc: 0.4755 - val_loss: 2.4793 - val_acc: 0.5548\n",
      "Epoch 2/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 2.2308 - acc: 0.6312 - val_loss: 2.0943 - val_acc: 0.6833\n",
      "Epoch 3/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 2.0160 - acc: 0.6995 - val_loss: 2.1725 - val_acc: 0.6661\n",
      "Epoch 4/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.8745 - acc: 0.7445 - val_loss: 1.9197 - val_acc: 0.7328\n",
      "Epoch 5/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.7728 - acc: 0.7696 - val_loss: 1.8014 - val_acc: 0.7696\n",
      "Epoch 6/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 1.6828 - acc: 0.7933 - val_loss: 1.8484 - val_acc: 0.7406\n",
      "Epoch 7/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 1.6032 - acc: 0.8109 - val_loss: 1.6740 - val_acc: 0.7964\n",
      "Epoch 8/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 1.5402 - acc: 0.8233 - val_loss: 1.6939 - val_acc: 0.7820\n",
      "Epoch 9/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 1.4786 - acc: 0.8353 - val_loss: 1.5724 - val_acc: 0.8109\n",
      "Epoch 10/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 1.4252 - acc: 0.8452 - val_loss: 1.5375 - val_acc: 0.8137\n",
      "Epoch 11/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.3701 - acc: 0.8566 - val_loss: 1.5632 - val_acc: 0.8094\n",
      "Epoch 12/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 1.3160 - acc: 0.8684 - val_loss: 1.4336 - val_acc: 0.8311\n",
      "Epoch 13/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.2809 - acc: 0.8721 - val_loss: 1.6013 - val_acc: 0.7922\n",
      "Epoch 14/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.2329 - acc: 0.8802 - val_loss: 1.4319 - val_acc: 0.8297\n",
      "Epoch 15/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.1969 - acc: 0.8858 - val_loss: 1.4319 - val_acc: 0.8254\n",
      "Epoch 16/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 1.1590 - acc: 0.8910 - val_loss: 1.3572 - val_acc: 0.8403\n",
      "Epoch 17/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 1.1196 - acc: 0.8987 - val_loss: 1.3755 - val_acc: 0.8328\n",
      "Epoch 18/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 1.0873 - acc: 0.9039 - val_loss: 1.3024 - val_acc: 0.8439\n",
      "Epoch 19/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 1.0552 - acc: 0.9080 - val_loss: 1.2606 - val_acc: 0.8504\n",
      "Epoch 20/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 1.0280 - acc: 0.9113 - val_loss: 1.2519 - val_acc: 0.8521\n",
      "Epoch 21/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 1.0002 - acc: 0.9153 - val_loss: 1.3268 - val_acc: 0.8362\n",
      "Epoch 22/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.9716 - acc: 0.9188 - val_loss: 1.2031 - val_acc: 0.8617\n",
      "Epoch 23/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.9444 - acc: 0.9244 - val_loss: 1.2017 - val_acc: 0.8592\n",
      "Epoch 24/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.9233 - acc: 0.9243 - val_loss: 1.1455 - val_acc: 0.8668\n",
      "Epoch 25/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.8949 - acc: 0.9294 - val_loss: 1.1984 - val_acc: 0.8578\n",
      "Epoch 26/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.8806 - acc: 0.9297 - val_loss: 1.1447 - val_acc: 0.8627\n",
      "Epoch 27/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.8506 - acc: 0.9356 - val_loss: 1.2153 - val_acc: 0.8497\n",
      "Epoch 28/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.8385 - acc: 0.9343 - val_loss: 1.1805 - val_acc: 0.8539\n",
      "Epoch 29/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.8152 - acc: 0.9384 - val_loss: 1.1405 - val_acc: 0.8643\n",
      "Epoch 30/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.7944 - acc: 0.9423 - val_loss: 1.0968 - val_acc: 0.8679\n",
      "Epoch 31/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.7762 - acc: 0.9432 - val_loss: 1.1148 - val_acc: 0.8644\n",
      "Epoch 32/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.7554 - acc: 0.9461 - val_loss: 1.0491 - val_acc: 0.8732\n",
      "Epoch 33/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.7393 - acc: 0.9482 - val_loss: 1.0701 - val_acc: 0.8645\n",
      "Epoch 34/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.7249 - acc: 0.9495 - val_loss: 1.0841 - val_acc: 0.8666\n",
      "Epoch 35/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.7183 - acc: 0.9482 - val_loss: 1.0623 - val_acc: 0.8652\n",
      "Epoch 36/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6945 - acc: 0.9518 - val_loss: 1.0322 - val_acc: 0.8762\n",
      "Epoch 37/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6911 - acc: 0.9506 - val_loss: 1.0504 - val_acc: 0.8708\n",
      "Epoch 38/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.6708 - acc: 0.9538 - val_loss: 1.0090 - val_acc: 0.8709\n",
      "Epoch 39/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6515 - acc: 0.9575 - val_loss: 1.0125 - val_acc: 0.8763\n",
      "Epoch 40/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.6354 - acc: 0.9594 - val_loss: 1.0439 - val_acc: 0.8723\n",
      "Epoch 41/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6379 - acc: 0.9554 - val_loss: 1.0956 - val_acc: 0.8617\n",
      "Epoch 42/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6253 - acc: 0.9567 - val_loss: 0.9704 - val_acc: 0.8803\n",
      "Epoch 43/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6058 - acc: 0.9597 - val_loss: 1.0521 - val_acc: 0.8599\n",
      "Epoch 44/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.6001 - acc: 0.9597 - val_loss: 0.9282 - val_acc: 0.8842\n",
      "Epoch 45/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.5882 - acc: 0.9608 - val_loss: 0.9741 - val_acc: 0.8723\n",
      "Epoch 46/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.5787 - acc: 0.9622 - val_loss: 0.9506 - val_acc: 0.8793\n",
      "Epoch 47/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.5638 - acc: 0.9648 - val_loss: 1.0018 - val_acc: 0.8664\n",
      "Epoch 48/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.5605 - acc: 0.9619 - val_loss: 0.9552 - val_acc: 0.8745\n",
      "Epoch 49/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.5541 - acc: 0.9621 - val_loss: 0.9420 - val_acc: 0.8789\n",
      "Epoch 50/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.5376 - acc: 0.9659 - val_loss: 0.9522 - val_acc: 0.8795\n",
      "Epoch 51/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.5348 - acc: 0.9638 - val_loss: 0.9877 - val_acc: 0.8623\n",
      "Epoch 52/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.5200 - acc: 0.9669 - val_loss: 0.9737 - val_acc: 0.8765\n",
      "Epoch 53/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.5210 - acc: 0.9641 - val_loss: 0.9176 - val_acc: 0.8804\n",
      "Epoch 54/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.5082 - acc: 0.9663 - val_loss: 0.9337 - val_acc: 0.8728\n",
      "Epoch 55/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.5044 - acc: 0.9664 - val_loss: 0.9723 - val_acc: 0.8655\n",
      "Epoch 56/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.4974 - acc: 0.9669 - val_loss: 0.9011 - val_acc: 0.8792\n",
      "Epoch 57/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.4862 - acc: 0.9687 - val_loss: 0.9026 - val_acc: 0.8793\n",
      "Epoch 58/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.4817 - acc: 0.9672 - val_loss: 0.8462 - val_acc: 0.8863\n",
      "Epoch 59/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4723 - acc: 0.9691 - val_loss: 0.8890 - val_acc: 0.8796\n",
      "Epoch 60/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4727 - acc: 0.9674 - val_loss: 0.9043 - val_acc: 0.8822\n",
      "Epoch 61/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4635 - acc: 0.9692 - val_loss: 0.8788 - val_acc: 0.8836\n",
      "Epoch 62/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4606 - acc: 0.9684 - val_loss: 0.8872 - val_acc: 0.8787\n",
      "Epoch 63/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4451 - acc: 0.9709 - val_loss: 0.8682 - val_acc: 0.8842\n",
      "Epoch 64/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.4464 - acc: 0.9698 - val_loss: 0.8372 - val_acc: 0.8892\n",
      "Epoch 65/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.4359 - acc: 0.9723 - val_loss: 0.8539 - val_acc: 0.8870\n",
      "Epoch 66/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4364 - acc: 0.9708 - val_loss: 0.8309 - val_acc: 0.8895\n",
      "Epoch 67/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4307 - acc: 0.9713 - val_loss: 0.8855 - val_acc: 0.8796\n",
      "Epoch 68/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.4206 - acc: 0.9725 - val_loss: 0.8595 - val_acc: 0.8837\n",
      "Epoch 69/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4195 - acc: 0.9715 - val_loss: 0.8212 - val_acc: 0.8860\n",
      "Epoch 70/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4164 - acc: 0.9716 - val_loss: 0.9285 - val_acc: 0.8706\n",
      "Epoch 71/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4126 - acc: 0.9709 - val_loss: 0.9166 - val_acc: 0.8767\n",
      "Epoch 72/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4080 - acc: 0.9716 - val_loss: 0.8490 - val_acc: 0.8843\n",
      "Epoch 73/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3953 - acc: 0.9751 - val_loss: 0.7932 - val_acc: 0.8873\n",
      "Epoch 74/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3951 - acc: 0.9739 - val_loss: 0.8455 - val_acc: 0.8772\n",
      "Epoch 75/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.4004 - acc: 0.9707 - val_loss: 0.8090 - val_acc: 0.8898\n",
      "Epoch 76/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3904 - acc: 0.9731 - val_loss: 0.7888 - val_acc: 0.8888\n",
      "Epoch 77/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3865 - acc: 0.9734 - val_loss: 0.8138 - val_acc: 0.8876\n",
      "Epoch 78/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3776 - acc: 0.9748 - val_loss: 0.8532 - val_acc: 0.8843\n",
      "Epoch 79/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.3764 - acc: 0.9742 - val_loss: 0.7932 - val_acc: 0.8888\n",
      "Epoch 80/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3808 - acc: 0.9719 - val_loss: 0.8489 - val_acc: 0.8800\n",
      "Epoch 81/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3642 - acc: 0.9775 - val_loss: 0.7848 - val_acc: 0.8885\n",
      "Epoch 82/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3701 - acc: 0.9737 - val_loss: 0.8203 - val_acc: 0.8821\n",
      "Epoch 83/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3670 - acc: 0.9736 - val_loss: 0.7962 - val_acc: 0.8806\n",
      "Epoch 84/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3727 - acc: 0.9708 - val_loss: 0.8541 - val_acc: 0.8816\n",
      "Epoch 85/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.3695 - acc: 0.9710 - val_loss: 0.8064 - val_acc: 0.8845\n",
      "Epoch 86/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3504 - acc: 0.9780 - val_loss: 0.8374 - val_acc: 0.8754\n",
      "Epoch 87/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3488 - acc: 0.9771 - val_loss: 0.7934 - val_acc: 0.8836\n",
      "Epoch 88/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3536 - acc: 0.9744 - val_loss: 0.8624 - val_acc: 0.8730\n",
      "Epoch 89/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3387 - acc: 0.9789 - val_loss: 0.7514 - val_acc: 0.8934\n",
      "Epoch 90/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3477 - acc: 0.9749 - val_loss: 0.8355 - val_acc: 0.8830\n",
      "Epoch 91/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3487 - acc: 0.9739 - val_loss: 0.7642 - val_acc: 0.8850\n",
      "Epoch 92/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3351 - acc: 0.9776 - val_loss: 0.8821 - val_acc: 0.8737\n",
      "Epoch 93/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3374 - acc: 0.9769 - val_loss: 0.7409 - val_acc: 0.8927\n",
      "Epoch 94/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3389 - acc: 0.9743 - val_loss: 0.7512 - val_acc: 0.8891\n",
      "Epoch 95/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3304 - acc: 0.9768 - val_loss: 0.7713 - val_acc: 0.8853\n",
      "Epoch 96/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3338 - acc: 0.9755 - val_loss: 0.7477 - val_acc: 0.8926\n",
      "Epoch 97/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3221 - acc: 0.9781 - val_loss: 0.7735 - val_acc: 0.8875\n",
      "Epoch 98/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3315 - acc: 0.9747 - val_loss: 0.7955 - val_acc: 0.8841\n",
      "Epoch 99/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.3217 - acc: 0.9774 - val_loss: 0.7717 - val_acc: 0.8873\n",
      "Epoch 100/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.3233 - acc: 0.9763 - val_loss: 0.7597 - val_acc: 0.8829\n",
      "Epoch 101/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2833 - acc: 0.9904 - val_loss: 0.6303 - val_acc: 0.9138\n",
      "Epoch 102/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2677 - acc: 0.9964 - val_loss: 0.6231 - val_acc: 0.9155\n",
      "Epoch 103/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2637 - acc: 0.9974 - val_loss: 0.6238 - val_acc: 0.9179\n",
      "Epoch 104/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2599 - acc: 0.9984 - val_loss: 0.6196 - val_acc: 0.9170\n",
      "Epoch 105/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2583 - acc: 0.9983 - val_loss: 0.6198 - val_acc: 0.9186\n",
      "Epoch 106/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2570 - acc: 0.9987 - val_loss: 0.6214 - val_acc: 0.9193\n",
      "Epoch 107/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2554 - acc: 0.9989 - val_loss: 0.6159 - val_acc: 0.9207\n",
      "Epoch 108/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2542 - acc: 0.9991 - val_loss: 0.6178 - val_acc: 0.9203\n",
      "Epoch 109/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.2527 - acc: 0.9991 - val_loss: 0.6296 - val_acc: 0.9178\n",
      "Epoch 110/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2515 - acc: 0.9995 - val_loss: 0.6194 - val_acc: 0.9205\n",
      "Epoch 111/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2505 - acc: 0.9994 - val_loss: 0.6208 - val_acc: 0.9206\n",
      "Epoch 112/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.2499 - acc: 0.9992 - val_loss: 0.6146 - val_acc: 0.9222\n",
      "Epoch 113/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2490 - acc: 0.9993 - val_loss: 0.6246 - val_acc: 0.9203\n",
      "Epoch 114/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2479 - acc: 0.9994 - val_loss: 0.6209 - val_acc: 0.9225\n",
      "Epoch 115/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2470 - acc: 0.9995 - val_loss: 0.6201 - val_acc: 0.9222\n",
      "Epoch 116/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2459 - acc: 0.9995 - val_loss: 0.6177 - val_acc: 0.9194\n",
      "Epoch 117/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.2452 - acc: 0.9996 - val_loss: 0.6236 - val_acc: 0.9201\n",
      "Epoch 118/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2443 - acc: 0.9995 - val_loss: 0.6245 - val_acc: 0.9211\n",
      "Epoch 119/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2434 - acc: 0.9995 - val_loss: 0.6172 - val_acc: 0.9207\n",
      "Epoch 120/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2422 - acc: 0.9997 - val_loss: 0.6199 - val_acc: 0.9216\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2415 - acc: 0.9996 - val_loss: 0.6240 - val_acc: 0.9202\n",
      "Epoch 122/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.2407 - acc: 0.9997 - val_loss: 0.6234 - val_acc: 0.9215\n",
      "Epoch 123/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2401 - acc: 0.9995 - val_loss: 0.6129 - val_acc: 0.9225\n",
      "Epoch 124/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2392 - acc: 0.9997 - val_loss: 0.6223 - val_acc: 0.9217\n",
      "Epoch 125/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2382 - acc: 0.9998 - val_loss: 0.6110 - val_acc: 0.9227\n",
      "Epoch 126/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2376 - acc: 0.9996 - val_loss: 0.6163 - val_acc: 0.9229\n",
      "Epoch 127/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2366 - acc: 0.9998 - val_loss: 0.6158 - val_acc: 0.9219\n",
      "Epoch 128/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2360 - acc: 0.9997 - val_loss: 0.6145 - val_acc: 0.9231\n",
      "Epoch 129/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2352 - acc: 0.9997 - val_loss: 0.6177 - val_acc: 0.9228\n",
      "Epoch 130/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.2343 - acc: 0.9998 - val_loss: 0.6185 - val_acc: 0.9218\n",
      "Epoch 131/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2339 - acc: 0.9997 - val_loss: 0.6218 - val_acc: 0.9216\n",
      "Epoch 132/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2326 - acc: 0.9999 - val_loss: 0.6140 - val_acc: 0.9229\n",
      "Epoch 133/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2322 - acc: 0.9997 - val_loss: 0.6157 - val_acc: 0.9223\n",
      "Epoch 134/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2314 - acc: 0.9997 - val_loss: 0.6129 - val_acc: 0.9231\n",
      "Epoch 135/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2305 - acc: 0.9998 - val_loss: 0.6170 - val_acc: 0.9223\n",
      "Epoch 136/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2298 - acc: 0.9999 - val_loss: 0.6156 - val_acc: 0.9218\n",
      "Epoch 137/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2291 - acc: 0.9997 - val_loss: 0.6177 - val_acc: 0.9224\n",
      "Epoch 138/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2283 - acc: 0.9999 - val_loss: 0.6184 - val_acc: 0.9221\n",
      "Epoch 139/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2278 - acc: 0.9998 - val_loss: 0.6168 - val_acc: 0.9227\n",
      "Epoch 140/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.2269 - acc: 0.9998 - val_loss: 0.6160 - val_acc: 0.9228\n",
      "Epoch 141/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2261 - acc: 0.9998 - val_loss: 0.6127 - val_acc: 0.9231\n",
      "Epoch 142/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2257 - acc: 0.9998 - val_loss: 0.6153 - val_acc: 0.9239\n",
      "Epoch 143/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2247 - acc: 0.9998 - val_loss: 0.6141 - val_acc: 0.9238\n",
      "Epoch 144/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2240 - acc: 0.9998 - val_loss: 0.6169 - val_acc: 0.9232\n",
      "Epoch 145/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2232 - acc: 0.9999 - val_loss: 0.6211 - val_acc: 0.9216\n",
      "Epoch 146/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2227 - acc: 0.9999 - val_loss: 0.6127 - val_acc: 0.9224\n",
      "Epoch 147/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2219 - acc: 0.9998 - val_loss: 0.6174 - val_acc: 0.9220\n",
      "Epoch 148/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2209 - acc: 0.9999 - val_loss: 0.6168 - val_acc: 0.9230\n",
      "Epoch 149/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2204 - acc: 0.9998 - val_loss: 0.6171 - val_acc: 0.9222\n",
      "Epoch 150/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.2197 - acc: 0.9999 - val_loss: 0.6204 - val_acc: 0.9227\n",
      "Epoch 151/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2190 - acc: 0.9999 - val_loss: 0.6208 - val_acc: 0.9231\n",
      "Epoch 152/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.2182 - acc: 0.9999 - val_loss: 0.6204 - val_acc: 0.9228\n",
      "Epoch 153/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.2176 - acc: 0.9999 - val_loss: 0.6171 - val_acc: 0.9238\n",
      "Epoch 154/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2168 - acc: 0.9999 - val_loss: 0.6150 - val_acc: 0.9230\n",
      "Epoch 155/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2160 - acc: 1.0000 - val_loss: 0.6184 - val_acc: 0.9231\n",
      "Epoch 156/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2154 - acc: 0.9999 - val_loss: 0.6067 - val_acc: 0.9246\n",
      "Epoch 157/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2147 - acc: 0.9999 - val_loss: 0.6086 - val_acc: 0.9233\n",
      "Epoch 158/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2140 - acc: 0.9999 - val_loss: 0.6116 - val_acc: 0.9231\n",
      "Epoch 159/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2135 - acc: 0.9999 - val_loss: 0.6045 - val_acc: 0.9243\n",
      "Epoch 160/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2127 - acc: 0.9999 - val_loss: 0.6124 - val_acc: 0.9227\n",
      "Epoch 161/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2122 - acc: 0.9998 - val_loss: 0.6137 - val_acc: 0.9223\n",
      "Epoch 162/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2113 - acc: 0.9999 - val_loss: 0.6156 - val_acc: 0.9213\n",
      "Epoch 163/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2106 - acc: 0.9999 - val_loss: 0.6121 - val_acc: 0.9232\n",
      "Epoch 164/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2102 - acc: 0.9998 - val_loss: 0.6159 - val_acc: 0.9221\n",
      "Epoch 165/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2094 - acc: 0.9999 - val_loss: 0.6074 - val_acc: 0.9233\n",
      "Epoch 166/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2090 - acc: 0.9998 - val_loss: 0.6096 - val_acc: 0.9232\n",
      "Epoch 167/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2085 - acc: 0.9998 - val_loss: 0.6185 - val_acc: 0.9214\n",
      "Epoch 168/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2076 - acc: 0.9998 - val_loss: 0.6167 - val_acc: 0.9211\n",
      "Epoch 169/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2068 - acc: 0.9999 - val_loss: 0.6103 - val_acc: 0.9232\n",
      "Epoch 170/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2061 - acc: 0.9999 - val_loss: 0.6150 - val_acc: 0.9215\n",
      "Epoch 171/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2055 - acc: 0.9999 - val_loss: 0.6108 - val_acc: 0.9213\n",
      "Epoch 172/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2049 - acc: 0.9999 - val_loss: 0.6085 - val_acc: 0.9221\n",
      "Epoch 173/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2040 - acc: 1.0000 - val_loss: 0.6106 - val_acc: 0.9242\n",
      "Epoch 174/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2035 - acc: 0.9999 - val_loss: 0.6081 - val_acc: 0.9234\n",
      "Epoch 175/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2028 - acc: 1.0000 - val_loss: 0.6089 - val_acc: 0.9234\n",
      "Epoch 176/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.2023 - acc: 0.9998 - val_loss: 0.6039 - val_acc: 0.9252\n",
      "Epoch 177/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2018 - acc: 0.9998 - val_loss: 0.6086 - val_acc: 0.9256\n",
      "Epoch 178/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.2011 - acc: 0.9999 - val_loss: 0.6102 - val_acc: 0.9250\n",
      "Epoch 179/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.2005 - acc: 0.9999 - val_loss: 0.6010 - val_acc: 0.9239\n",
      "Epoch 180/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1997 - acc: 1.0000 - val_loss: 0.6065 - val_acc: 0.9229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1991 - acc: 0.9999 - val_loss: 0.6065 - val_acc: 0.9235\n",
      "Epoch 182/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1986 - acc: 0.9999 - val_loss: 0.6106 - val_acc: 0.9233\n",
      "Epoch 183/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1978 - acc: 1.0000 - val_loss: 0.6084 - val_acc: 0.9242\n",
      "Epoch 184/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1972 - acc: 0.9999 - val_loss: 0.6122 - val_acc: 0.9229\n",
      "Epoch 185/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1965 - acc: 0.9999 - val_loss: 0.6000 - val_acc: 0.9247\n",
      "Epoch 186/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1959 - acc: 1.0000 - val_loss: 0.6053 - val_acc: 0.9242\n",
      "Epoch 187/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1953 - acc: 0.9999 - val_loss: 0.5982 - val_acc: 0.9240\n",
      "Epoch 188/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1947 - acc: 0.9999 - val_loss: 0.6009 - val_acc: 0.9251\n",
      "Epoch 189/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1942 - acc: 0.9999 - val_loss: 0.6033 - val_acc: 0.9237\n",
      "Epoch 190/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1936 - acc: 0.9999 - val_loss: 0.6024 - val_acc: 0.9245\n",
      "Epoch 191/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1928 - acc: 1.0000 - val_loss: 0.6017 - val_acc: 0.9246\n",
      "Epoch 192/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1923 - acc: 1.0000 - val_loss: 0.6030 - val_acc: 0.9238\n",
      "Epoch 193/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1916 - acc: 1.0000 - val_loss: 0.6032 - val_acc: 0.9237\n",
      "Epoch 194/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1911 - acc: 1.0000 - val_loss: 0.6038 - val_acc: 0.9237\n",
      "Epoch 195/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1906 - acc: 0.9999 - val_loss: 0.6041 - val_acc: 0.9245\n",
      "Epoch 196/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1898 - acc: 1.0000 - val_loss: 0.6020 - val_acc: 0.9243\n",
      "Epoch 197/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1893 - acc: 0.9998 - val_loss: 0.6067 - val_acc: 0.9227\n",
      "Epoch 198/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1887 - acc: 1.0000 - val_loss: 0.6010 - val_acc: 0.9240\n",
      "Epoch 199/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1881 - acc: 1.0000 - val_loss: 0.6009 - val_acc: 0.9233\n",
      "Epoch 200/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1874 - acc: 1.0000 - val_loss: 0.6004 - val_acc: 0.9239\n",
      "Epoch 201/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1873 - acc: 0.9999 - val_loss: 0.6040 - val_acc: 0.9235\n",
      "Epoch 202/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1873 - acc: 0.9999 - val_loss: 0.5997 - val_acc: 0.9245\n",
      "Epoch 203/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1870 - acc: 0.9999 - val_loss: 0.5946 - val_acc: 0.9250\n",
      "Epoch 204/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1869 - acc: 1.0000 - val_loss: 0.5993 - val_acc: 0.9236\n",
      "Epoch 205/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1870 - acc: 0.9999 - val_loss: 0.6020 - val_acc: 0.9234\n",
      "Epoch 206/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1868 - acc: 1.0000 - val_loss: 0.5984 - val_acc: 0.9253\n",
      "Epoch 207/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1869 - acc: 0.9998 - val_loss: 0.5984 - val_acc: 0.9240\n",
      "Epoch 208/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1867 - acc: 1.0000 - val_loss: 0.5914 - val_acc: 0.9243\n",
      "Epoch 209/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1867 - acc: 1.0000 - val_loss: 0.5960 - val_acc: 0.9239\n",
      "Epoch 210/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1867 - acc: 0.9999 - val_loss: 0.5955 - val_acc: 0.9247\n",
      "Epoch 211/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1866 - acc: 0.9999 - val_loss: 0.5927 - val_acc: 0.9251\n",
      "Epoch 212/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1866 - acc: 0.9999 - val_loss: 0.5991 - val_acc: 0.9240\n",
      "Epoch 213/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1866 - acc: 0.9999 - val_loss: 0.5956 - val_acc: 0.9247\n",
      "Epoch 214/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1864 - acc: 1.0000 - val_loss: 0.5933 - val_acc: 0.9248\n",
      "Epoch 215/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1864 - acc: 0.9999 - val_loss: 0.5997 - val_acc: 0.9243\n",
      "Epoch 216/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1863 - acc: 0.9999 - val_loss: 0.5940 - val_acc: 0.9241\n",
      "Epoch 217/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1862 - acc: 1.0000 - val_loss: 0.5914 - val_acc: 0.9240\n",
      "Epoch 218/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1862 - acc: 0.9999 - val_loss: 0.5990 - val_acc: 0.9236\n",
      "Epoch 219/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1861 - acc: 1.0000 - val_loss: 0.5986 - val_acc: 0.9232\n",
      "Epoch 220/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1860 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.9247\n",
      "Epoch 221/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1859 - acc: 1.0000 - val_loss: 0.5972 - val_acc: 0.9244\n",
      "Epoch 222/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1859 - acc: 1.0000 - val_loss: 0.5954 - val_acc: 0.9234\n",
      "Epoch 223/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1860 - acc: 0.9999 - val_loss: 0.5997 - val_acc: 0.9240\n",
      "Epoch 224/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1857 - acc: 1.0000 - val_loss: 0.5953 - val_acc: 0.9250\n",
      "Epoch 225/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1858 - acc: 1.0000 - val_loss: 0.5907 - val_acc: 0.9252\n",
      "Epoch 226/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1857 - acc: 1.0000 - val_loss: 0.6023 - val_acc: 0.9232\n",
      "Epoch 227/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1855 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.9239\n",
      "Epoch 228/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1856 - acc: 0.9999 - val_loss: 0.5925 - val_acc: 0.9242\n",
      "Epoch 229/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1856 - acc: 0.9999 - val_loss: 0.5962 - val_acc: 0.9250\n",
      "Epoch 230/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1855 - acc: 1.0000 - val_loss: 0.5905 - val_acc: 0.9240\n",
      "Epoch 231/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1853 - acc: 1.0000 - val_loss: 0.5966 - val_acc: 0.9239\n",
      "Epoch 232/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1854 - acc: 0.9999 - val_loss: 0.5923 - val_acc: 0.9249\n",
      "Epoch 233/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1853 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.9238\n",
      "Epoch 234/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1852 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.9241\n",
      "Epoch 235/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1852 - acc: 0.9999 - val_loss: 0.6008 - val_acc: 0.9226\n",
      "Epoch 236/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1851 - acc: 1.0000 - val_loss: 0.5986 - val_acc: 0.9237\n",
      "Epoch 237/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1851 - acc: 1.0000 - val_loss: 0.5910 - val_acc: 0.9241\n",
      "Epoch 238/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1849 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.9237\n",
      "Epoch 239/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1850 - acc: 1.0000 - val_loss: 0.5954 - val_acc: 0.9234\n",
      "Epoch 240/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1849 - acc: 1.0000 - val_loss: 0.5959 - val_acc: 0.9230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1850 - acc: 0.9999 - val_loss: 0.5969 - val_acc: 0.9240\n",
      "Epoch 242/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1847 - acc: 1.0000 - val_loss: 0.5913 - val_acc: 0.9250\n",
      "Epoch 243/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1847 - acc: 1.0000 - val_loss: 0.5935 - val_acc: 0.9235\n",
      "Epoch 244/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1846 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.9244\n",
      "Epoch 245/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1845 - acc: 1.0000 - val_loss: 0.5998 - val_acc: 0.9229\n",
      "Epoch 246/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1846 - acc: 1.0000 - val_loss: 0.6014 - val_acc: 0.9236\n",
      "Epoch 247/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1845 - acc: 1.0000 - val_loss: 0.5931 - val_acc: 0.9252\n",
      "Epoch 248/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1845 - acc: 0.9999 - val_loss: 0.5971 - val_acc: 0.9239\n",
      "Epoch 249/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1844 - acc: 0.9999 - val_loss: 0.5957 - val_acc: 0.9244\n",
      "Epoch 250/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1844 - acc: 0.9999 - val_loss: 0.6006 - val_acc: 0.9240\n",
      "Epoch 251/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1843 - acc: 1.0000 - val_loss: 0.5934 - val_acc: 0.9238\n",
      "Epoch 252/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1844 - acc: 0.9999 - val_loss: 0.5983 - val_acc: 0.9244\n",
      "Epoch 253/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1843 - acc: 0.9999 - val_loss: 0.5958 - val_acc: 0.9247\n",
      "Epoch 254/300\n",
      "782/782 [==============================] - 67s 85ms/step - loss: 0.1842 - acc: 1.0000 - val_loss: 0.5973 - val_acc: 0.9247\n",
      "Epoch 255/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1840 - acc: 1.0000 - val_loss: 0.5947 - val_acc: 0.9241\n",
      "Epoch 256/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1840 - acc: 1.0000 - val_loss: 0.5976 - val_acc: 0.9233\n",
      "Epoch 257/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1840 - acc: 1.0000 - val_loss: 0.5951 - val_acc: 0.9241\n",
      "Epoch 258/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1839 - acc: 1.0000 - val_loss: 0.5992 - val_acc: 0.9226\n",
      "Epoch 259/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1839 - acc: 0.9999 - val_loss: 0.5966 - val_acc: 0.9245\n",
      "Epoch 260/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1838 - acc: 1.0000 - val_loss: 0.5945 - val_acc: 0.9225\n",
      "Epoch 261/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1837 - acc: 1.0000 - val_loss: 0.6001 - val_acc: 0.9223\n",
      "Epoch 262/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1836 - acc: 1.0000 - val_loss: 0.5943 - val_acc: 0.9234\n",
      "Epoch 263/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1836 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.9238\n",
      "Epoch 264/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1835 - acc: 1.0000 - val_loss: 0.5998 - val_acc: 0.9231\n",
      "Epoch 265/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1836 - acc: 1.0000 - val_loss: 0.5912 - val_acc: 0.9240\n",
      "Epoch 266/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1834 - acc: 1.0000 - val_loss: 0.5948 - val_acc: 0.9237\n",
      "Epoch 267/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1833 - acc: 1.0000 - val_loss: 0.5941 - val_acc: 0.9234\n",
      "Epoch 268/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1834 - acc: 0.9999 - val_loss: 0.6019 - val_acc: 0.9238\n",
      "Epoch 269/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1832 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.9240\n",
      "Epoch 270/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1832 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.9247\n",
      "Epoch 271/300\n",
      "782/782 [==============================] - 70s 89ms/step - loss: 0.1832 - acc: 1.0000 - val_loss: 0.5931 - val_acc: 0.9238\n",
      "Epoch 272/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1831 - acc: 0.9999 - val_loss: 0.5982 - val_acc: 0.9242\n",
      "Epoch 273/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1830 - acc: 1.0000 - val_loss: 0.5977 - val_acc: 0.9239\n",
      "Epoch 274/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1832 - acc: 0.9998 - val_loss: 0.5893 - val_acc: 0.9245\n",
      "Epoch 275/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1830 - acc: 0.9999 - val_loss: 0.5995 - val_acc: 0.9235\n",
      "Epoch 276/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1828 - acc: 1.0000 - val_loss: 0.5956 - val_acc: 0.9240\n",
      "Epoch 277/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1827 - acc: 1.0000 - val_loss: 0.5885 - val_acc: 0.9241\n",
      "Epoch 278/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1828 - acc: 0.9999 - val_loss: 0.5957 - val_acc: 0.9247\n",
      "Epoch 279/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1827 - acc: 1.0000 - val_loss: 0.5998 - val_acc: 0.9239\n",
      "Epoch 280/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1827 - acc: 0.9999 - val_loss: 0.5874 - val_acc: 0.9239\n",
      "Epoch 281/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1826 - acc: 0.9999 - val_loss: 0.5976 - val_acc: 0.9238\n",
      "Epoch 282/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1825 - acc: 0.9999 - val_loss: 0.5940 - val_acc: 0.9235\n",
      "Epoch 283/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1824 - acc: 1.0000 - val_loss: 0.5933 - val_acc: 0.9232\n",
      "Epoch 284/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1824 - acc: 1.0000 - val_loss: 0.5955 - val_acc: 0.9230\n",
      "Epoch 285/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1824 - acc: 1.0000 - val_loss: 0.5940 - val_acc: 0.9244\n",
      "Epoch 286/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1822 - acc: 1.0000 - val_loss: 0.5899 - val_acc: 0.9239\n",
      "Epoch 287/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1822 - acc: 1.0000 - val_loss: 0.5951 - val_acc: 0.9246\n",
      "Epoch 288/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1822 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.9243\n",
      "Epoch 289/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1822 - acc: 1.0000 - val_loss: 0.5936 - val_acc: 0.9241\n",
      "Epoch 290/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1821 - acc: 0.9999 - val_loss: 0.5923 - val_acc: 0.9238\n",
      "Epoch 291/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1819 - acc: 1.0000 - val_loss: 0.5955 - val_acc: 0.9240\n",
      "Epoch 292/300\n",
      "782/782 [==============================] - 69s 88ms/step - loss: 0.1819 - acc: 1.0000 - val_loss: 0.5927 - val_acc: 0.9230\n",
      "Epoch 293/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1820 - acc: 1.0000 - val_loss: 0.5932 - val_acc: 0.9244\n",
      "Epoch 294/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1818 - acc: 1.0000 - val_loss: 0.5952 - val_acc: 0.9237\n",
      "Epoch 295/300\n",
      "782/782 [==============================] - 67s 86ms/step - loss: 0.1818 - acc: 1.0000 - val_loss: 0.5919 - val_acc: 0.9237\n",
      "Epoch 296/300\n",
      "782/782 [==============================] - 68s 86ms/step - loss: 0.1817 - acc: 1.0000 - val_loss: 0.5955 - val_acc: 0.9236\n",
      "Epoch 297/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1817 - acc: 0.9999 - val_loss: 0.5968 - val_acc: 0.9231\n",
      "Epoch 298/300\n",
      "782/782 [==============================] - 70s 90ms/step - loss: 0.1816 - acc: 1.0000 - val_loss: 0.5921 - val_acc: 0.9221\n",
      "Epoch 299/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1816 - acc: 1.0000 - val_loss: 0.5937 - val_acc: 0.9240\n",
      "Epoch 300/300\n",
      "782/782 [==============================] - 68s 87ms/step - loss: 0.1815 - acc: 1.0000 - val_loss: 0.5945 - val_acc: 0.9247\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# set callback\n",
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]\n",
    "\n",
    "# set data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.125,\n",
    "                             height_shift_range=0.125,\n",
    "                             fill_mode='constant',cval=0.)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# start training\n",
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('xception_he_wd_slim_thinner.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
